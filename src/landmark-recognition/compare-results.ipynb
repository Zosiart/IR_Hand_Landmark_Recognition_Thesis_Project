{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-29T10:44:17.534268Z",
     "start_time": "2024-12-29T10:44:11.602760Z"
    }
   },
   "source": [
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "from rembg import remove\n",
    "from PIL import Image\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Parameters",
   "id": "1a49b1954276f7f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:00.535169Z",
     "start_time": "2024-12-29T12:24:00.519555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mediapipe_model_path = '../hand_landmarker.task'\n",
    "mediapipe_gesture_recognizer_model_path = '../gesture_recognizer.task'\n",
    "RGB_image_directory = '../../resources/evaluation_dataset/RGB'\n",
    "RGB_ground_truth_directory = '../../resources/evaluation_dataset/RGB_annotation'\n",
    "IR_image_directory = '../../resources/evaluation_dataset/IR_rotated'\n",
    "IR_ground_truth_directory = '../../resources/evaluation_dataset/IR_rotated_annotations'\n",
    "bad_score_penalty = 1\n",
    "distance_threshold = 0.07"
   ],
   "id": "3489d579e81a5cc9",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create Models",
   "id": "9ee8b2ca26ced1a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:01.242012Z",
     "start_time": "2024-12-29T12:24:01.152343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_options = python.BaseOptions(model_asset_path=mediapipe_gesture_recognizer_model_path)\n",
    "options = vision.GestureRecognizerOptions(base_options=base_options,\n",
    "                                          num_hands=2)\n",
    "\n",
    "options.canned_gesture_classifier_options.score_threshold = 0\n",
    "gesture_recognizer = vision.GestureRecognizer.create_from_options(options)"
   ],
   "id": "5b272bc566577fe5",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:01.291229Z",
     "start_time": "2024-12-29T12:24:01.278248Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "91d5d87de4166488",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Drawing Landmarks",
   "id": "10fd4b7d9e0fb4f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:01.556982Z",
     "start_time": "2024-12-29T12:24:01.525708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MARGIN = 10  # pixels\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "HANDEDNESS_TEXT_COLOR = (88, 205, 54) # vibrant green\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "  hand_landmarks_list = detection_result.hand_landmarks\n",
    "  handedness_list = detection_result.handedness\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  # Loop through the detected hands to visualize.\n",
    "  for idx in range(len(hand_landmarks_list)):\n",
    "    hand_landmarks = hand_landmarks_list[idx]\n",
    "    handedness = handedness_list[idx]\n",
    "\n",
    "    # Draw the hand landmarks.\n",
    "    hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    hand_landmarks_proto.landmark.extend([\n",
    "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
    "    ])\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "      annotated_image,\n",
    "      hand_landmarks_proto,\n",
    "      solutions.hands.HAND_CONNECTIONS,\n",
    "      solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "      solutions.drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "    # Get the top left corner of the detected hand's bounding box.\n",
    "    height, width, _ = annotated_image.shape\n",
    "    x_coordinates = [landmark.x for landmark in hand_landmarks]\n",
    "    y_coordinates = [landmark.y for landmark in hand_landmarks]\n",
    "    text_x = int(min(x_coordinates) * width)\n",
    "    text_y = int(min(y_coordinates) * height) - MARGIN\n",
    "\n",
    "    # Draw handedness (left or right hand) on the image.\n",
    "    cv2.putText(annotated_image, f\"{handedness[0].category_name}\",\n",
    "                (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
    "                FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n",
    "\n",
    "  return annotated_image"
   ],
   "id": "2427619c14560a50",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:01.682908Z",
     "start_time": "2024-12-29T12:24:01.666819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MARGIN = 10  # pixels\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "HANDEDNESS_TEXT_COLOR = (88, 205, 54) # vibrant green\n",
    "\n",
    "def draw_landmarks_on_image_from_json(rgb_image, evaluation_result):\n",
    "  hand_landmarks_list = evaluation_result['landmarks']\n",
    "  # handedness_list = detection_result.handedness\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  # Loop through the detected hands to visualize.\n",
    "  for idx in range(len(hand_landmarks_list)):\n",
    "    hand_landmarks = hand_landmarks_list[idx]\n",
    "    # handedness = handedness_list[idx]\n",
    "    height, width, _ = annotated_image.shape\n",
    "    # Draw the hand landmarks.\n",
    "    hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    hand_landmarks_proto.landmark.extend([\n",
    "      landmark_pb2.NormalizedLandmark(x=float(landmark['x']), y=float(landmark['y']), z=0.0) for landmark in hand_landmarks\n",
    "    ])\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "      annotated_image,\n",
    "      hand_landmarks_proto,\n",
    "      solutions.hands.HAND_CONNECTIONS,\n",
    "      solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "      solutions.drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "    # Get the top left corner of the detected hand's bounding box.\n",
    "   \n",
    "    x_coordinates = [landmark['x'] for landmark in hand_landmarks]\n",
    "    y_coordinates = [landmark['y'] for landmark in hand_landmarks]\n",
    "    text_x = int(min(x_coordinates) * width)\n",
    "    text_y = int(min(y_coordinates) * height) - MARGIN\n",
    "\n",
    "    # Draw handedness (left or right hand) on the image.\n",
    "    cv2.putText(annotated_image, f\"Left\",\n",
    "                (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
    "                FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n",
    "\n",
    "  return annotated_image"
   ],
   "id": "8c59b8f6478c6ff9",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Assessing the Accuracy",
   "id": "a89c5474010875a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:01.994987Z",
     "start_time": "2024-12-29T12:24:01.983934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_accuracy(ground_truth, prediction, threshold):\n",
    "    if prediction is None:  # Missing landmark\n",
    "        return 0.0\n",
    "    distance = np.sqrt((float(ground_truth[\"x\"]) - prediction.x) ** 2 + \n",
    "                       (float(ground_truth[\"y\"]) - prediction.y) ** 2)\n",
    "    return max(0, 1 - distance / threshold) * 100"
   ],
   "id": "25bfd4e1071d2f8f",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:02.132215Z",
     "start_time": "2024-12-29T12:24:02.115036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from itertools import permutations\n",
    "\n",
    "def calculate_hand_accuracies(ground_truth_hands, predicted_hands, handedness_info, threshold):\n",
    "    accuracies = {\"Left\": [], \"Right\": []}\n",
    "\n",
    "    # If no hands detected, accuracy is 0% for both hands\n",
    "    if len(predicted_hands) == 0:\n",
    "        return {\"Left\": [0.0]*21, \"Right\": [0.0]*21}\n",
    "    \n",
    "\n",
    "    first_predicted_hand = predicted_hands[0]\n",
    "\n",
    "    hand_accuracy_1 = [\n",
    "        calculate_accuracy(gt_landmark, pred_landmark, threshold)\n",
    "        for gt_landmark, pred_landmark in zip(ground_truth_hands[0], first_predicted_hand)\n",
    "    ]\n",
    "\n",
    "    hand_accuracy_2 = [\n",
    "        calculate_accuracy(gt_landmark, pred_landmark, threshold)\n",
    "        for gt_landmark, pred_landmark in zip(ground_truth_hands[1], first_predicted_hand)\n",
    "    ]\n",
    "\n",
    "    if len(predicted_hands) == 1 and sum(hand_accuracy_1) > sum(hand_accuracy_2):\n",
    "        accuracies[\"Left\"] = hand_accuracy_1\n",
    "        accuracies[\"Right\"] = [0.0]*21\n",
    "        return accuracies\n",
    "    elif len(predicted_hands) == 1:\n",
    "        accuracies[\"Right\"] = hand_accuracy_2\n",
    "        accuracies[\"Left\"] = [0.0]*21\n",
    "        return accuracies\n",
    "    else:\n",
    "        second_predicted_hand = predicted_hands[1]\n",
    "        hand_accuracy_3 = [\n",
    "            calculate_accuracy(gt_landmark, pred_landmark, threshold)\n",
    "            for gt_landmark, pred_landmark in zip(ground_truth_hands[0], second_predicted_hand)\n",
    "        ]\n",
    "\n",
    "        hand_accuracy_4 = [\n",
    "            calculate_accuracy(gt_landmark, pred_landmark, threshold)\n",
    "            for gt_landmark, pred_landmark in zip(ground_truth_hands[1], second_predicted_hand)\n",
    "        ]\n",
    "\n",
    "        if sum(hand_accuracy_1) + sum(hand_accuracy_4) > sum(hand_accuracy_2) + sum(hand_accuracy_3):\n",
    "            accuracies[\"Left\"] = hand_accuracy_1\n",
    "            accuracies[\"Right\"] = hand_accuracy_4\n",
    "        else:\n",
    "            accuracies[\"Right\"] = hand_accuracy_2\n",
    "            accuracies[\"Left\"] = hand_accuracy_3\n",
    "\n",
    "    return accuracies"
   ],
   "id": "3e1e1b9685c123c5",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:02.281053Z",
     "start_time": "2024-12-29T12:24:02.274839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def assess_accuracy(image_path, ground_truth_hands):\n",
    "    \"\"\"\n",
    "    Assess accuracy of predictions for a given image.\n",
    "    \"\"\"\n",
    "    image = mp.Image.create_from_file(image_path)\n",
    "    \n",
    "    results = gesture_recognizer.recognize(image)\n",
    "    \n",
    "    if not results.hand_landmarks or len(results.hand_landmarks) == 0:\n",
    "        print(f\"No hands detected for {image_path}\")\n",
    "        return {\"Left\": [0.0]*21, \"Right\": [0.0]*21}\n",
    "\n",
    "    return calculate_hand_accuracies(\n",
    "        ground_truth_hands,\n",
    "        results.hand_landmarks,\n",
    "        results.handedness,\n",
    "        distance_threshold\n",
    "    )"
   ],
   "id": "ba74de54791fbfe1",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Calculating Distance and Average Sum of Distance for the Dataset",
   "id": "3b2cbb064e13dc48"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:02.568870Z",
     "start_time": "2024-12-29T12:24:02.561794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_euclidean_distance(ground_truth, prediction):\n",
    "    distance = np.sqrt((float(ground_truth[\"x\"]) - prediction.x) ** 2 + \n",
    "                       (float(ground_truth[\"y\"]) - prediction.y) ** 2)\n",
    "    return distance"
   ],
   "id": "bd037d6f1070e035",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:02.716041Z",
     "start_time": "2024-12-29T12:24:02.693624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_sum_of_distances(ground_truth_hands, predicted_hands):\n",
    "    distances = {\"Left\": [], \"Right\": []}\n",
    "    if len(predicted_hands) == 0:\n",
    "        return sum([bad_score_penalty*21]) + sum([bad_score_penalty*21])\n",
    "    \n",
    "    if len(predicted_hands) == 1:\n",
    "        print(\"Only one hand detected\")\n",
    "\n",
    "    first_predicted_hand = predicted_hands[0]\n",
    "\n",
    "    hand_distances_1 = [\n",
    "        calculate_euclidean_distance(gt_landmark, pred_landmark)\n",
    "        for gt_landmark, pred_landmark in zip(ground_truth_hands[0], first_predicted_hand)\n",
    "    ]\n",
    "\n",
    "    hand_distances_2 = [\n",
    "        calculate_euclidean_distance(gt_landmark, pred_landmark)\n",
    "        for gt_landmark, pred_landmark in zip(ground_truth_hands[1], first_predicted_hand)\n",
    "    ]\n",
    "\n",
    "    if len(predicted_hands) == 1 and sum(hand_distances_1) < sum(hand_distances_2):\n",
    "        distances[\"Left\"] = sum(hand_distances_1)\n",
    "        distances[\"Right\"] = sum([bad_score_penalty*21])\n",
    "    elif len(predicted_hands) == 1:\n",
    "        distances[\"Right\"] = sum(hand_distances_2)\n",
    "        distances[\"Left\"] = sum([bad_score_penalty*21])\n",
    "    else:\n",
    "        second_predicted_hand = predicted_hands[1]\n",
    "        hand_distances_3 = [\n",
    "            calculate_euclidean_distance(gt_landmark, pred_landmark)\n",
    "            for gt_landmark, pred_landmark in zip(ground_truth_hands[0], second_predicted_hand)\n",
    "        ]\n",
    "\n",
    "        hand_distances_4 = [\n",
    "            calculate_euclidean_distance(gt_landmark, pred_landmark)\n",
    "            for gt_landmark, pred_landmark in zip(ground_truth_hands[1], second_predicted_hand)\n",
    "        ]\n",
    "\n",
    "        if sum(hand_distances_1) + sum(hand_distances_4) < sum(hand_distances_2) + sum(hand_distances_3):\n",
    "            distances[\"Left\"] = sum(hand_distances_1)\n",
    "            distances[\"Right\"] = sum(hand_distances_4)\n",
    "        else:\n",
    "            distances[\"Right\"] = sum(hand_distances_2)\n",
    "            distances[\"Left\"] = sum(hand_distances_3)\n",
    "            \n",
    "    \n",
    "\n",
    "    return distances[\"Left\"] + distances[\"Right\"]"
   ],
   "id": "6a0b299e0819002c",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:02.836970Z",
     "start_time": "2024-12-29T12:24:02.821342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_sum_of_distances_for_image(image_path, ground_truth_hands):\n",
    "    image = mp.Image.create_from_file(image_path)\n",
    "    results = gesture_recognizer.recognize(image)\n",
    "    distance = calculate_sum_of_distances(ground_truth_hands, results.hand_landmarks)\n",
    "    \n",
    "    return distance"
   ],
   "id": "9b52478ced6d1e4f",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:02.988438Z",
     "start_time": "2024-12-29T12:24:02.956699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_distances(ground_truth_hands, predicted_hands):\n",
    "    distances = {\"Left\": [], \"Right\": []}\n",
    "    if len(predicted_hands) == 0:\n",
    "        return {\"Left\": [bad_score_penalty] * 21, \"Right\": [bad_score_penalty] * 21 }\n",
    "    \n",
    "    if len(predicted_hands) == 1:\n",
    "        print(\"Only one hand detected\")\n",
    "\n",
    "    first_predicted_hand = predicted_hands[0]\n",
    "\n",
    "    hand_distances_1 = [\n",
    "        calculate_euclidean_distance(gt_landmark, pred_landmark)\n",
    "        for gt_landmark, pred_landmark in zip(ground_truth_hands[0], first_predicted_hand)\n",
    "    ]\n",
    "\n",
    "    hand_distances_2 = [\n",
    "        calculate_euclidean_distance(gt_landmark, pred_landmark)\n",
    "        for gt_landmark, pred_landmark in zip(ground_truth_hands[1], first_predicted_hand)\n",
    "    ]\n",
    "\n",
    "    if len(predicted_hands) == 1 and sum(hand_distances_1) < sum(hand_distances_2):\n",
    "        distances[\"Left\"] = hand_distances_1\n",
    "        distances[\"Right\"] = [bad_score_penalty] * 21\n",
    "    elif len(predicted_hands) == 1:\n",
    "        distances[\"Right\"] = hand_distances_2\n",
    "        distances[\"Left\"] = [bad_score_penalty] * 21\n",
    "    else:\n",
    "        second_predicted_hand = predicted_hands[1]\n",
    "        hand_distances_3 = [\n",
    "            calculate_euclidean_distance(gt_landmark, pred_landmark)\n",
    "            for gt_landmark, pred_landmark in zip(ground_truth_hands[0], second_predicted_hand)\n",
    "        ]\n",
    "\n",
    "        hand_distances_4 = [\n",
    "            calculate_euclidean_distance(gt_landmark, pred_landmark)\n",
    "            for gt_landmark, pred_landmark in zip(ground_truth_hands[1], second_predicted_hand)\n",
    "        ]\n",
    "\n",
    "        if sum(hand_distances_1) + sum(hand_distances_4) < sum(hand_distances_2) + sum(hand_distances_3):\n",
    "            distances[\"Left\"] = hand_distances_1\n",
    "            distances[\"Right\"] = hand_distances_4\n",
    "        else:\n",
    "            distances[\"Right\"] = hand_distances_2\n",
    "            distances[\"Left\"] = hand_distances_3\n",
    "            \n",
    "    return distances"
   ],
   "id": "232c3da9a1202f3a",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:03.105969Z",
     "start_time": "2024-12-29T12:24:03.096983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_avg_distance_for_image(image_path, ground_truth_hands):\n",
    "    image = mp.Image.create_from_file(image_path)\n",
    "    results = gesture_recognizer.recognize(image)\n",
    "    distances = calculate_distances(ground_truth_hands, results.hand_landmarks)\n",
    "    \n",
    "    return sum(distances[\"Left\"] + distances[\"Right\"]) / len(distances[\"Left\"] + distances[\"Right\"])"
   ],
   "id": "17371f80dc4a2861",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:03.254873Z",
     "start_time": "2024-12-29T12:24:03.239228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_distances_for_image(image_path, ground_truth_hands):\n",
    "    image = mp.Image.create_from_file(image_path)\n",
    "    results = gesture_recognizer.recognize(image)\n",
    "    distances = calculate_distances(ground_truth_hands, results.hand_landmarks)\n",
    "    \n",
    "    return distances[\"Left\"] + distances[\"Right\"]"
   ],
   "id": "f5e1728539b74ead",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating Thresholds/Bounds\n",
   "id": "238d1ae0f27a83a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:03.559366Z",
     "start_time": "2024-12-29T12:24:03.555310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_final_accuracy(scores):\n",
    "    all_scores = scores[\"Left\"] + scores[\"Right\"]\n",
    "    return sum(all_scores) / len(all_scores) if all_scores else 0.0"
   ],
   "id": "cb280ec4155ddba6",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:03.708583Z",
     "start_time": "2024-12-29T12:24:03.703429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def calculate_bound_accuracy(image_directory, ground_truth_directory):\n",
    "    total_bound = {\"Left\": [], \"Right\":[]}\n",
    "    for file_name in os.listdir(ground_truth_directory):\n",
    "        if file_name.endswith('.json'):\n",
    "            json_file_path = os.path.join(ground_truth_directory, file_name)\n",
    "            \n",
    "            with open(json_file_path, 'r') as f:\n",
    "                ground_truth_data = json.load(f)\n",
    "                \n",
    "            for entry in ground_truth_data:\n",
    "                image_name = entry[\"image\"]\n",
    "                ground_truth_landmarks = entry[\"landmarks\"]\n",
    "                image_path = os.path.join(image_directory, image_name)\n",
    "    \n",
    "                # Assess accuracy for RGB images (upper bound)\n",
    "                score = assess_accuracy(image_path, ground_truth_landmarks)\n",
    "                \n",
    "                for hand in [\"Left\", \"Right\"]:\n",
    "                    total_bound[hand].extend(score[hand])\n",
    "    return calculate_final_accuracy(total_bound)\n",
    "    "
   ],
   "id": "fd1d97cdedc6a2c5",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:15.041873Z",
     "start_time": "2024-12-29T12:24:03.854637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "upper_bound_accuracy = calculate_bound_accuracy(RGB_image_directory, RGB_ground_truth_directory)\n",
    "lower_bound_accuracy = calculate_bound_accuracy(IR_image_directory ,IR_ground_truth_directory)"
   ],
   "id": "53d9cc0ef275495a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hands detected for ../../resources/evaluation_dataset/IR_rotated\\10_IMG20241127100740.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR_rotated\\11_IMG20241127100906.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR_rotated\\12_IMG20241127100913.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR_rotated\\13_IMG20241127101012.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR_rotated\\14_IMG20241127101019.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR_rotated\\23_IMG20241127101749.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR_rotated\\29_IMG20241127102051.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR_rotated\\30_IMG20241127102058.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR_rotated\\38_IMG20241127102643.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR_rotated\\40_IMG20241127102805.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR_rotated\\41_IMG20241127102811.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR_rotated\\42_IMG20241127102854.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR_rotated\\45_IMG20241127103113.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR_rotated\\49_IMG20241127103207.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR_rotated\\54_IMG20241127103409.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR_rotated\\56_IMG20241127103449.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR_rotated\\6_IMG20241127100515.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR_rotated\\75_IMG20241127104020.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR_rotated\\78_IMG20241127105250.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR_rotated\\79_IMG20241127105256.jpg\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:15.074188Z",
     "start_time": "2024-12-29T12:24:15.059081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Upper Bound Accuracy (RGB): {upper_bound_accuracy:.2f}%\")\n",
    "print(f\"Lower Bound Accuracy (Infrared): {lower_bound_accuracy:.2f}%\")"
   ],
   "id": "68bfde699ca4b2af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upper Bound Accuracy (RGB): 88.76%\n",
      "Lower Bound Accuracy (Infrared): 43.67%\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualizing Thresholds",
   "id": "5a02e8544a65e6d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:15.121868Z",
     "start_time": "2024-12-29T12:24:15.106220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def visualize_threshold_roi(image_path, ground_truth_landmarks, threshold, output_path):\n",
    "    \"\"\"\n",
    "    Visualizes regions of interest (ROIs) based on the threshold for accuracy.\n",
    "\n",
    "    Parameters:\n",
    "        image_path (str): Path to the image file.\n",
    "        ground_truth_landmarks (list of dict): Ground truth landmarks with 'x' and 'y' coordinates.\n",
    "        threshold (float): Distance threshold for accuracy calculation.\n",
    "        output_path (str): Path to save the output visualization image.\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    overlay = image.copy()\n",
    "\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    # Draw circles around the ground truth landmarks\n",
    "    for hand in ground_truth_landmarks:\n",
    "        for landmark in hand:\n",
    "            center = (int(float(landmark['x']) * width), int(float(landmark['y']) * height))\n",
    "            radius = int(threshold*width)  # Scale threshold to image dimensions\n",
    "            color = (0, 255, 0)  # Green color for the circle\n",
    "            alpha = 0.4  # Transparency factor\n",
    "    \n",
    "            # Draw the circle on the overlay\n",
    "            cv2.circle(overlay, center, radius, color, -1)\n",
    "\n",
    "    # Blend the overlay with the original image\n",
    "    cv2.addWeighted(overlay, alpha, image, 1 - alpha, 0, image)\n",
    "\n",
    "    # Save the output image\n",
    "    cv2.imwrite(output_path, image)"
   ],
   "id": "8d34912e9f7d4ff0",
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Transforming Images",
   "id": "27b48a742b6daddb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:15.149422Z",
     "start_time": "2024-12-29T12:24:15.135739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../colorization/zhang')"
   ],
   "id": "beffe87da273403a",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:15.181277Z",
     "start_time": "2024-12-29T12:24:15.165205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.colorization.Zhang import create_colorized_pictures\n",
    "def colorize_zhang_eccv16(image_path, image_name):\n",
    "    create_colorized_pictures(model='eccv16', img_path=image_path, save_prefix=image_name)  \n",
    "    return f'../../resources/stylized-pictures/eccv16/{image_name}_eccv16.png'"
   ],
   "id": "64d3a755aeb977dc",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:15.213198Z",
     "start_time": "2024-12-29T12:24:15.197014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def colorize_zhang_siggraph17(image_path, image_name):\n",
    "    create_colorized_pictures(model='siggraph17', img_path=image_path, save_prefix=image_name)  \n",
    "    return f'../../resources/stylized-pictures/siggraph17/{image_name}_siggraph17.png'\n",
    "    "
   ],
   "id": "fbd9306a4ef46991",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:15.245135Z",
     "start_time": "2024-12-29T12:24:15.229296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clahe(image, clipLimit=2.0, tileGridSize=(8, 8), save_path=None, save_as_rgb=True):\n",
    "    \"\"\"\n",
    "    Applies Contrast Limited Adaptive Histogram Equalization (CLAHE) to an image.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: Input image (grayscale or BGR/RGB).\n",
    "    - clipLimit: Threshold for contrast limiting.\n",
    "    - tileGridSize: Size of the grid for histogram equalization.\n",
    "    - save_path: Path to save the output image.\n",
    "    - save_as_rgb: Save the CLAHE-applied image in RGB format if True.\n",
    "    \n",
    "    Returns:\n",
    "    - Processed image in grayscale or RGB format.\n",
    "    \"\"\"\n",
    "    # Convert to grayscale if it's a color image\n",
    "    if len(image.shape) == 3:  # If RGB/BGR\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = image\n",
    "\n",
    "    # Apply CLAHE\n",
    "    clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)\n",
    "    clahe_image = clahe.apply(gray)\n",
    "\n",
    "    # Convert back to RGB if required\n",
    "    if save_as_rgb:\n",
    "        clahe_image_rgb = cv2.cvtColor(clahe_image, cv2.COLOR_GRAY2RGB)\n",
    "    else:\n",
    "        clahe_image_rgb = clahe_image\n",
    "\n",
    "    # Save the image if a save_path is provided\n",
    "    if save_path:\n",
    "        cv2.imwrite(save_path, clahe_image_rgb if save_as_rgb else clahe_image)\n",
    "        print(f\"CLAHE-applied image saved at {save_path}\")\n",
    "\n",
    "    return clahe_image_rgb if save_as_rgb else clahe_image\n"
   ],
   "id": "cb8da00b65162670",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:15.276878Z",
     "start_time": "2024-12-29T12:24:15.261220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def reduce_brightness(image_path, save_path, reduction_factor=0.5):\n",
    "    \"\"\"\n",
    "    Reads an image, reduces its brightness, and saves the result.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_path: Path to the input image.\n",
    "    - save_path: Path to save the output image.\n",
    "    - reduction_factor: Factor by which to reduce brightness (0.0 to 1.0, where 1.0 is no change).\n",
    "    \n",
    "    Returns:\n",
    "    - The brightness-reduced image as a NumPy array.\n",
    "    \"\"\"\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Image at path {image_path} could not be loaded.\")\n",
    "\n",
    "    # Convert to float to avoid clipping during multiplication\n",
    "    image = image.astype(np.float32)\n",
    "\n",
    "    # Reduce brightness\n",
    "    brightness_reduced = np.clip(image * reduction_factor, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # Save the result\n",
    "    cv2.imwrite(save_path, brightness_reduced)\n",
    "    print(f\"Brightness-reduced image saved at {save_path}\")\n",
    "\n",
    "    return brightness_reduced\n"
   ],
   "id": "481263388286cb23",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:15.308641Z",
     "start_time": "2024-12-29T12:24:15.293649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_background(image_path, save_path):\n",
    "    # Processing the image \n",
    "    input = Image.open(image_path)\n",
    "    \n",
    "    # Removing the background from the given Image \n",
    "    output = remove(input)\n",
    "    \n",
    "    # Saving the image in the given path\n",
    "    output.save(save_path)"
   ],
   "id": "f7ff38527823b67f",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:15.340115Z",
     "start_time": "2024-12-29T12:24:15.324318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gaussian_smoothing(image, kernel_size=(5, 5), sigma=0):\n",
    "    \"\"\"\n",
    "    Applies Gaussian smoothing to an image.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: Input image (grayscale or BGR/RGB).\n",
    "    - kernel_size: Size of the Gaussian kernel (height, width).\n",
    "    - sigma: Standard deviation of the Gaussian distribution.\n",
    "    \n",
    "    Returns:\n",
    "    - Smoothed image.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(f\"../../resources/stylized-pictures/no_background/{image_name}_background_removed.png\")\n",
    "    # Apply median blur\n",
    "    median_blur = cv2.medianBlur(image, 5)\n",
    "    #smoothed = cv2.GaussianBlur(image, kernel_size, sigma)\n",
    "    # Save the smoothed image\n",
    "    cv2.imwrite(f\"../../resources/stylized-pictures/no_background/smoothed/{image_name}_smoothed.png\", median_blur)\n",
    "    return f\"../../resources/stylized-pictures/no_background/smoothed/{image_name}_smoothed.png\"\n",
    "    "
   ],
   "id": "28d5f43487d7f754",
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:15.371829Z",
     "start_time": "2024-12-29T12:24:15.357599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def remove_temperature_boxes(image_path, image_name):\n",
    "    # Load the image using OpenCV\n",
    "    image_cv = cv2.imread(image_path)\n",
    "\n",
    "    # Convert to HSV to target specific colors (red and green boxes)\n",
    "    hsv_image = cv2.cvtColor(image_cv, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Define color ranges for red and green (in HSV space)\n",
    "    lower_red1 = np.array([0, 120, 70])\n",
    "    upper_red1 = np.array([10, 255, 255])\n",
    "    lower_red2 = np.array([170, 120, 70])\n",
    "    upper_red2 = np.array([180, 255, 255])\n",
    "    lower_green = np.array([35, 100, 100])\n",
    "    upper_green = np.array([85, 255, 255])\n",
    "\n",
    "    # Create masks for red and green\n",
    "    mask_red1 = cv2.inRange(hsv_image, lower_red1, upper_red1)\n",
    "    mask_red2 = cv2.inRange(hsv_image, lower_red2, upper_red2)\n",
    "    mask_green = cv2.inRange(hsv_image, lower_green, upper_green)\n",
    "    mask_red = mask_red1 | mask_red2\n",
    "\n",
    "    # Combine masks for both colors\n",
    "    mask_combined = mask_red | mask_green\n",
    "\n",
    "    # Dilate the mask slightly to cover edges of the red and green regions\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    dilated_mask = cv2.dilate(mask_combined, kernel, iterations=1)\n",
    "\n",
    "    # Inpaint to remove red and green regions\n",
    "    image_no_boxes = cv2.inpaint(image_cv, dilated_mask, inpaintRadius=5, flags=cv2.INPAINT_TELEA)\n",
    "\n",
    "    # Apply Gaussian blur to smooth the entire image\n",
    "    smoothed_image = cv2.GaussianBlur(image_no_boxes, (5, 5), 0)\n",
    "\n",
    "    # Save the final image\n",
    "    smoothed_output_path = f\"../../resources/stylized-pictures/no_boxes/{image_name}_no_boxes.png\"\n",
    "    cv2.imwrite(smoothed_output_path, smoothed_image)\n",
    "\n",
    "    return smoothed_output_path\n",
    "\n"
   ],
   "id": "e971e56c123f0501",
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:15.403893Z",
     "start_time": "2024-12-29T12:24:15.388381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def mask_temperature_scale(image_path, output_path):\n",
    "    \"\"\"\n",
    "    Masks the temperature scale on the left side of the image\n",
    "    by replacing it with the average background color.\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Define the regions to mask\n",
    "    scale_width = 210  # Width of the scale region on the left\n",
    "    temp_box_width = 260  # Width of the temperature box in the top-right\n",
    "    temp_box_height = 90  # Height of the temperature box in the top-right\n",
    "\n",
    "    # Mask the left temperature scale\n",
    "    scale_region = image[:, :scale_width]\n",
    "    avg_color_scale = np.mean(image[:, scale_width:], axis=(0, 1), dtype=int)\n",
    "    image[:, :scale_width] = avg_color_scale\n",
    "\n",
    "    # Mask the temperature box in the upper-right corner\n",
    "    temp_box_region = image[:temp_box_height, -temp_box_width:]\n",
    "    avg_color_temp_box = np.mean(image[temp_box_height:, :-temp_box_width], axis=(0, 1), dtype=int)\n",
    "    image[:temp_box_height, -temp_box_width:] = avg_color_temp_box\n",
    "\n",
    "     # Save the masked image\n",
    "    cv2.imwrite(output_path, image)\n",
    "\n",
    "    return output_path"
   ],
   "id": "9074bbc0da338447",
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:15.435174Z",
     "start_time": "2024-12-29T12:24:15.419535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def detect_edges_sobelX(image):\n",
    "    \"\"\"Detects edges in an image using the Sobel edge detection algorithm\"\"\"\n",
    "    return cv2.Sobel(image,cv2.CV_64F,1,0,ksize=5)\n",
    "\n",
    "def detect_edges_sobelY(image):\n",
    "    \"\"\"Detects edges in an image using the Sobel edge detection algorithm\"\"\"\n",
    "    return cv2.Sobel(image,cv2.CV_64F,0,1,ksize=5)"
   ],
   "id": "84866f49999f39ca",
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:15.466591Z",
     "start_time": "2024-12-29T12:24:15.450789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def canny_edge_detection(frame): \n",
    "    # Convert the frame to grayscale for edge detection \n",
    "    # gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) \n",
    "      \n",
    "    # Apply Gaussian blur to reduce noise and smoothen edges \n",
    "    blurred = cv2.GaussianBlur(src=frame, ksize=(3, 5), sigmaX=0.5) \n",
    "      \n",
    "    # Perform Canny edge detection \n",
    "    edges = cv2.Canny(blurred, 70, 135) \n",
    "      \n",
    "    return blurred, edges"
   ],
   "id": "608c4352109ebf66",
   "outputs": [],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:15.498393Z",
     "start_time": "2024-12-29T12:24:15.482375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check whether recognized hands are left or right\n",
    "def is_right_hand(landmarks):\n",
    "    counter = 0\n",
    "    \n",
    "    for landmark in landmarks:\n",
    "        if landmark.x > 0.5:\n",
    "            counter += 1\n",
    "            \n",
    "    return counter > 10"
   ],
   "id": "ea710006e172b12e",
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Data",
   "id": "4a58957232e564bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:15.530120Z",
     "start_time": "2024-12-29T12:24:15.514325Z"
    }
   },
   "cell_type": "code",
   "source": "results = []",
   "id": "b47374467ec521f6",
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Processing Images",
   "id": "b60bd7a4774aa302"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T12:24:15.561415Z",
     "start_time": "2024-12-29T12:24:15.545773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from rembg import remove \n",
    "from PIL import Image"
   ],
   "id": "3391c11d5b67060b",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T13:27:59.160526Z",
     "start_time": "2024-12-29T13:27:59.144889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_cross(image_path, image_name):\n",
    "    \"\"\"\n",
    "    Removes the white cross from the center of an image by creating a mask and applying inpainting.\n",
    "    \"\"\"\n",
    "    # Load the image in color\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Get image dimensions\n",
    "    h, w = image.shape[:2]\n",
    "    center_x, center_y = w // 2, h // 2\n",
    "    \n",
    "    # Define cross size (adjust as needed)\n",
    "    cross_size = 50  # Length of the cross arms\n",
    "    thickness = 10   # Thickness of the lines\n",
    "\n",
    "    # Create a mask for the cross\n",
    "    mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    # Draw horizontal and vertical lines for the cross\n",
    "    cv2.line(mask, (center_x - cross_size, center_y), (center_x + cross_size, center_y), 255, thickness)\n",
    "    cv2.line(mask, (center_x, center_y - cross_size), (center_x, center_y + cross_size), 255, thickness)\n",
    "\n",
    "    # Apply inpainting to remove the cross\n",
    "    result = cv2.inpaint(image, mask, inpaintRadius=5, flags=cv2.INPAINT_TELEA)\n",
    "    \n",
    "    path = f\"../../resources/stylized-pictures/no_cross/{image_name}_no_cross.png\"\n",
    "    cv2.imwrite(path, result)\n",
    "    \n",
    "    return path\n"
   ],
   "id": "67d188352ad94c68",
   "outputs": [],
   "execution_count": 154
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T13:40:14.258714Z",
     "start_time": "2024-12-29T13:40:14.227427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def execute_transformation_pipeline(image_path, image_name, colorization_model='siggraph17'):\n",
    "    no_boxes_image_path = remove_temperature_boxes(image_path, image_name)\n",
    "    \n",
    "    # # Invert the colors\n",
    "    inverted_image = cv2.bitwise_not(cv2.imread(no_boxes_image_path))\n",
    "    # Save the inverted image\n",
    "    inverted_image_path = f\"../../resources/stylized-pictures/inverted/{image_name}_inverted.png\"\n",
    "    cv2.imwrite(inverted_image_path, inverted_image)\n",
    "\n",
    "    \n",
    "    # Generate colorized image using Zhang model\n",
    "    transformed_image_path = None\n",
    "\n",
    "    if colorization_model == 'eccv16':\n",
    "        transformed_image_path = colorize_zhang_eccv16(inverted_image_path, image_name)\n",
    "    elif colorization_model == 'siggraph17':\n",
    "        transformed_image_path = colorize_zhang_siggraph17(inverted_image_path, image_name)\n",
    "    \n",
    "    return transformed_image_path\n",
    "    "
   ],
   "id": "c1db65c787b519f2",
   "outputs": [],
   "execution_count": 180
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T13:40:14.810463Z",
     "start_time": "2024-12-29T13:40:14.795434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def execute_transformation_pipeline_with_no_visible_fingers(image_path, image_name, colorization_model='siggraph17'):\n",
    "    no_boxes_image_path = remove_temperature_boxes(image_path, image_name)\n",
    "    # no_cross_path = remove_cross(no_boxes_image_path, image_name)\n",
    "    \n",
    "    image = cv2.imread(no_boxes_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Step 3: Enhance contrast using CLAHE\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    enhanced_image = clahe.apply(image)\n",
    "    \n",
    "    # Step 4: Sharpen edges to highlight fingers\n",
    "    kernel = np.array([[-1, -1, -1],\n",
    "                       [-1,  9, -1],\n",
    "                       [-1, -1, -1]])\n",
    "    sharpened_image = cv2.filter2D(enhanced_image, -1, kernel)\n",
    "    \n",
    "    # Change the image to RGB format\n",
    "    sharpened_image = cv2.cvtColor(sharpened_image, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    sharpened_image_path = f\"../../resources/stylized-pictures/not_detected/sharpened/{image_name}_sharpened.png\"\n",
    "    cv2.imwrite(sharpened_image_path, sharpened_image)\n",
    "    \n",
    "    # transformed_image_path = colorize_zhang_siggraph17(sharpened_image_path, image_name)\n",
    "    \n",
    "    return sharpened_image_path\n",
    "    "
   ],
   "id": "77c8625a2aac3113",
   "outputs": [],
   "execution_count": 181
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T13:40:15.255722Z",
     "start_time": "2024-12-29T13:40:15.240094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def two_hands_detected(image_path):\n",
    "    image = mp.Image.create_from_file(image_path)\n",
    "    \n",
    "    results = gesture_recognizer.recognize(image)\n",
    "    \n",
    "    if not results.hand_landmarks or len(results.hand_landmarks) < 2:\n",
    "        return False\n",
    "    return True"
   ],
   "id": "a01c03a8ea31e254",
   "outputs": [],
   "execution_count": 182
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T13:40:15.428615Z",
     "start_time": "2024-12-29T13:40:15.413002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_landmarks_from_results(results):\n",
    "    landmarks = results.hand_landmarks\n",
    "    final_landmarks = {\"Left\": [], \"Right\": []}\n",
    "    for hand in landmarks:\n",
    "        if is_right_hand(hand):\n",
    "            final_landmarks[\"Right\"] = hand\n",
    "        else:\n",
    "            final_landmarks[\"Left\"] = hand\n",
    "    return final_landmarks\n",
    "    "
   ],
   "id": "9ac257bc11291c7a",
   "outputs": [],
   "execution_count": 183
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T13:40:15.666787Z",
     "start_time": "2024-12-29T13:40:15.651164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def annotate_landmarks(image_path, results, output_path):\n",
    "    try:\n",
    "        image = cv2.imread(image_path)\n",
    "        annotated_image = np.copy(image)\n",
    "        \n",
    "        for hand in results:\n",
    "            hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "            hand_landmarks_proto.landmark.extend([\n",
    "                landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand\n",
    "            ])\n",
    "            solutions.drawing_utils.draw_landmarks(\n",
    "                annotated_image,\n",
    "                hand_landmarks_proto,\n",
    "                solutions.hands.HAND_CONNECTIONS,\n",
    "                solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "                solutions.drawing_styles.get_default_hand_connections_style()\n",
    "            )\n",
    "        \n",
    "        cv2.imwrite(output_path, annotated_image)\n",
    "    except Exception as e:\n",
    "        print(f\"COULD NOT ANNOTATE LANDMARKS FOR {image_path.upper()}: {str(e).upper()}\")\n"
   ],
   "id": "6152a51b1ad97a81",
   "outputs": [],
   "execution_count": 184
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T13:40:15.818344Z",
     "start_time": "2024-12-29T13:40:15.802678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_transformed_siggraph17 = {\"Left\": [], \"Right\": []}\n",
    "total_transformed_eccv16 = {\"Left\": [], \"Right\": []}"
   ],
   "id": "654ccec865b177e5",
   "outputs": [],
   "execution_count": 185
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T13:40:16.185757Z",
     "start_time": "2024-12-29T13:40:16.170120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_distances_siggaph17 = []\n",
    "total_distances_eccv16 = []"
   ],
   "id": "c37319ec0fa2f07",
   "outputs": [],
   "execution_count": 186
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T13:40:16.362881Z",
     "start_time": "2024-12-29T13:40:16.347257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hand_landmarker_first = {\"Left\": [], \"Right\": []}\n",
    "hand_landmarker_second = {\"Left\": [], \"Right\": []}\n",
    "\n",
    "gesture_recognizer_first = {\"Left\": [], \"Right\": []}\n",
    "gesture_recognizer_second = {\"Left\": [], \"Right\": []}"
   ],
   "id": "e45e013960952490",
   "outputs": [],
   "execution_count": 187
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T13:44:07.706334Z",
     "start_time": "2024-12-29T13:40:16.525734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Process each image\n",
    "for file_name in os.listdir(IR_ground_truth_directory):\n",
    "    if file_name.endswith('.json'):\n",
    "        json_file_path = os.path.join(IR_ground_truth_directory, file_name)\n",
    "        \n",
    "        with open(json_file_path, 'r') as f:\n",
    "            ground_truth_data = json.load(f)\n",
    "            \n",
    "        for entry in ground_truth_data:\n",
    "            image_name = entry[\"image\"]\n",
    "            ground_truth_landmarks = entry[\"landmarks\"]\n",
    "            image_path = f\"{IR_image_directory}/{image_name}\"\n",
    "            \n",
    "            print(f\"Processing image: {image_name}\")\n",
    "\n",
    "            first_pipeline_image_path = execute_transformation_pipeline(image_path, image_name)\n",
    "            first_pipeline_score = assess_accuracy(first_pipeline_image_path, ground_truth_landmarks)\n",
    "            print(f\"Results for original pipeline - Left: {np.mean(first_pipeline_score['Left']):.2f}%, Right: {np.mean(first_pipeline_score['Right']):.2f}%\")\n",
    "            \n",
    "            second_pipeline_image_path = execute_transformation_pipeline_with_no_visible_fingers(image_path, image_name)\n",
    "            second_pipeline_score = assess_accuracy(second_pipeline_image_path, ground_truth_landmarks)\n",
    "            print(f\"Results for second pipeline - Left: {np.mean(second_pipeline_score['Left']):.2f}%, Right: {np.mean(second_pipeline_score['Right']):.2f}%\")\n",
    "            \n",
    "            image = mp.Image.create_from_file(first_pipeline_image_path)\n",
    "            recognizer_results_first_pipeline = gesture_recognizer.recognize(image)\n",
    "            landmarks_first_pipeline = create_landmarks_from_results(recognizer_results_first_pipeline)\n",
    "            \n",
    "            image = mp.Image.create_from_file(second_pipeline_image_path)\n",
    "            recognizer_results_second_pipeline = gesture_recognizer.recognize(image)\n",
    "            landmarks_second_pipeline = create_landmarks_from_results(recognizer_results_second_pipeline)\n",
    "            \n",
    "            final_score = first_pipeline_score\n",
    "            final_landmarks = []\n",
    "            \n",
    "            if len(recognizer_results_first_pipeline.gestures) == 2 and len(recognizer_results_second_pipeline.gestures) == 2:\n",
    "                mapped_indexes = {\"Left\": 0, \"Right\": 1}\n",
    "                for hand in [\"Left\", \"Right\"]:\n",
    "                    index = mapped_indexes[hand]\n",
    "                    first_pipeline_gesture = recognizer_results_first_pipeline.gestures[index][0]\n",
    "                    second_pipeline_gesture = recognizer_results_second_pipeline.gestures[index][0]\n",
    "                    \n",
    "                    if first_pipeline_gesture.category_name == 'Open_Palm' and second_pipeline_gesture.category_name == 'Open_Palm':\n",
    "                        # Both detected open palm, take the one with higher confidence\n",
    "                        print(\"Both recognized open palm\")\n",
    "                        if first_pipeline_gesture.score > second_pipeline_gesture.score:\n",
    "                            final_score[hand] = first_pipeline_score[hand]\n",
    "                            final_landmarks.append(landmarks_first_pipeline[hand])\n",
    "                        else:\n",
    "                            final_score[hand] = second_pipeline_score[hand]\n",
    "                            final_landmarks.append(landmarks_second_pipeline[hand])\n",
    "                    elif first_pipeline_gesture.category_name == 'Open_Palm':\n",
    "                        print(\"First recognized open palm\")\n",
    "                        final_score[hand] = first_pipeline_score[hand]\n",
    "                        final_landmarks.append(landmarks_first_pipeline[hand])\n",
    "                    elif second_pipeline_gesture.category_name == 'Open_Palm':\n",
    "                        print(\"Second recognized open palm\")\n",
    "                        final_score[hand] = second_pipeline_score[hand]\n",
    "                        final_landmarks.append(landmarks_second_pipeline[hand])\n",
    "                    else:\n",
    "                        print(\"Neither recognized open palm\")\n",
    "                        if first_pipeline_gesture.score < second_pipeline_gesture.score:\n",
    "                            final_score[hand] = first_pipeline_score[hand]\n",
    "                            final_landmarks.append(landmarks_first_pipeline[hand])\n",
    "                        else:\n",
    "                            final_score = second_pipeline_score\n",
    "                            final_landmarks.append(landmarks_second_pipeline[hand])\n",
    "                        \n",
    "            if len(recognizer_results_first_pipeline.gestures) == 2 and len(recognizer_results_second_pipeline.gestures) == 1:\n",
    "                second_pipeline_handedness = \"Right\" if is_right_hand(recognizer_results_second_pipeline.hand_landmarks[0]) else \"Left\"\n",
    "                complementary_hand = \"Right\" if second_pipeline_handedness == \"Left\" else \"Left\"\n",
    "                index = 0 if second_pipeline_handedness == \"Left\" else 1\n",
    "                first_pipeline_gesture = recognizer_results_first_pipeline.gestures[index][0]\n",
    "                second_pipeline_gesture = recognizer_results_second_pipeline.gestures[0][0]\n",
    "                \n",
    "                if first_pipeline_gesture == 'Open_Palm' and second_pipeline_gesture == 'Open_Palm' and first_pipeline_gesture.score > second_pipeline_gesture.score:\n",
    "                    final_score = first_pipeline_score\n",
    "                    final_landmarks.append(landmarks_first_pipeline[\"Left\"])\n",
    "                    final_landmarks.append(landmarks_first_pipeline[\"Right\"])\n",
    "                elif first_pipeline_gesture == 'Open_Palm' and second_pipeline_gesture == 'Open_Palm' and first_pipeline_gesture.score < second_pipeline_gesture.score:\n",
    "                    final_score[second_pipeline_handedness] = second_pipeline_score[second_pipeline_handedness]\n",
    "                    final_landmarks.append(landmarks_second_pipeline[second_pipeline_handedness])\n",
    "                    final_landmarks.append(landmarks_first_pipeline[complementary_hand])\n",
    "                elif first_pipeline_gesture == 'Open_Palm':\n",
    "                    final_score = first_pipeline_score\n",
    "                    final_landmarks.append(landmarks_first_pipeline[\"Left\"])\n",
    "                    final_landmarks.append(landmarks_first_pipeline[\"Right\"])\n",
    "                elif second_pipeline_gesture == 'Open_Palm':\n",
    "                    final_score[second_pipeline_handedness] = second_pipeline_score[second_pipeline_handedness]\n",
    "                    final_landmarks.append(landmarks_second_pipeline[second_pipeline_handedness])\n",
    "                    final_landmarks.append(landmarks_first_pipeline[complementary_hand])\n",
    "                else: # None of them are open palm, take the None gesture with less score\n",
    "                    if first_pipeline_gesture.score < second_pipeline_gesture.score:\n",
    "                        final_score = first_pipeline_score\n",
    "                        final_landmarks.append(landmarks_first_pipeline[\"Left\"])\n",
    "                        final_landmarks.append(landmarks_first_pipeline[\"Right\"])\n",
    "                    else:\n",
    "                        final_score[second_pipeline_handedness] = second_pipeline_score[second_pipeline_handedness] \n",
    "                        final_landmarks.append(landmarks_second_pipeline[second_pipeline_handedness])\n",
    "                        final_landmarks.append(landmarks_first_pipeline[complementary_hand])\n",
    "                \n",
    "            if len(recognizer_results_first_pipeline.gestures) == 1 and len(recognizer_results_second_pipeline.gestures) == 2:\n",
    "                first_pipeline_handedness = \"Right\" if is_right_hand(recognizer_results_first_pipeline.hand_landmarks[0]) else \"Left\"\n",
    "                complementary_hand = \"Right\" if first_pipeline_handedness == \"Left\" else \"Left\"\n",
    "                index = 0 if first_pipeline_handedness == \"Left\" else 1\n",
    "                first_pipeline_gesture = recognizer_results_first_pipeline.gestures[0][0]\n",
    "                second_pipeline_gesture = recognizer_results_second_pipeline.gestures[index][0]\n",
    "                \n",
    "                final_score = second_pipeline_score \n",
    "                \n",
    "                if first_pipeline_gesture.category_name == 'Open_Palm' and second_pipeline_gesture.category_name == 'Open_Palm' and first_pipeline_gesture.score > second_pipeline_gesture.score:\n",
    "                    final_score[first_pipeline_handedness] = first_pipeline_score[first_pipeline_handedness]\n",
    "                    final_score[complementary_hand] = second_pipeline_score[complementary_hand]\n",
    "                    final_landmarks.append(landmarks_first_pipeline[first_pipeline_handedness])\n",
    "                    final_landmarks.append(landmarks_second_pipeline[complementary_hand])\n",
    "                    \n",
    "                elif first_pipeline_gesture.category_name == 'Open_Palm' and second_pipeline_gesture.category_name == 'Open_Palm' and first_pipeline_gesture.score < second_pipeline_gesture.score:\n",
    "                    final_score[first_pipeline_handedness] = second_pipeline_score[first_pipeline_handedness]\n",
    "                    final_score[complementary_hand] = second_pipeline_score[complementary_hand]\n",
    "                    final_landmarks.append(landmarks_second_pipeline[first_pipeline_handedness])\n",
    "                    final_landmarks.append(landmarks_second_pipeline[complementary_hand])\n",
    "                elif first_pipeline_gesture.category_name == 'Open_Palm':\n",
    "                    final_score[first_pipeline_handedness] = first_pipeline_score[first_pipeline_handedness]\n",
    "                    final_score[complementary_hand] = second_pipeline_score[complementary_hand]\n",
    "                    final_landmarks.append(landmarks_first_pipeline[first_pipeline_handedness])\n",
    "                    final_landmarks.append(landmarks_second_pipeline[complementary_hand])\n",
    "                elif second_pipeline_gesture.category_name == 'Open_Palm':\n",
    "                    final_score = second_pipeline_score\n",
    "                    final_landmarks.append(landmarks_second_pipeline[first_pipeline_handedness])\n",
    "                    final_landmarks.append(landmarks_second_pipeline[complementary_hand])\n",
    "                else:  # None of them are open palm, take the None gesture with less score\n",
    "                    if first_pipeline_gesture.score < second_pipeline_gesture.score:\n",
    "                        final_score[first_pipeline_handedness] = first_pipeline_score[first_pipeline_handedness]\n",
    "                        final_score[complementary_hand] = second_pipeline_score[complementary_hand]\n",
    "                        final_landmarks.append(landmarks_first_pipeline[first_pipeline_handedness])\n",
    "                        final_landmarks.append(landmarks_second_pipeline[complementary_hand])\n",
    "                    else:\n",
    "                        final_score = second_pipeline_score\n",
    "                        final_landmarks.append(landmarks_second_pipeline[first_pipeline_handedness])\n",
    "                        final_landmarks.append(landmarks_second_pipeline[complementary_hand])\n",
    "                        \n",
    "            if len(recognizer_results_first_pipeline.gestures) == 1 and len(recognizer_results_second_pipeline.gestures) == 1:\n",
    "                first_pipeline_handedness = \"Right\" if is_right_hand(recognizer_results_first_pipeline.hand_landmarks[0]) else \"Left\"\n",
    "                second_pipeline_handedness = \"Right\" if is_right_hand(recognizer_results_second_pipeline.hand_landmarks[0]) else \"Left\"\n",
    "                \n",
    "                if first_pipeline_handedness != second_pipeline_handedness:\n",
    "                    final_score[first_pipeline_handedness] = first_pipeline_score[first_pipeline_handedness]\n",
    "                    final_score[second_pipeline_handedness] = second_pipeline_score[second_pipeline_handedness]\n",
    "                    final_landmarks.append(landmarks_first_pipeline[first_pipeline_handedness])\n",
    "                    final_landmarks.append(landmarks_second_pipeline[second_pipeline_handedness])\n",
    "                else:\n",
    "                    first_pipeline_gesture = recognizer_results_first_pipeline.gestures[0][0]\n",
    "                    second_pipeline_gesture = recognizer_results_second_pipeline.gestures[0][0]\n",
    "                    \n",
    "                    if first_pipeline_gesture.category_name == 'Open_Palm' and second_pipeline_gesture.category_name == 'Open_Palm' and first_pipeline_gesture.score > second_pipeline_gesture.score:\n",
    "                        final_score = first_pipeline_score\n",
    "                        final_landmarks.append(landmarks_first_pipeline[first_pipeline_handedness])\n",
    "                    elif first_pipeline_gesture.category_name == 'Open_Palm' and second_pipeline_gesture.category_name == 'Open_Palm' and first_pipeline_gesture.score < second_pipeline_gesture.score:\n",
    "                        final_score = second_pipeline_score\n",
    "                        final_landmarks.append(landmarks_second_pipeline[second_pipeline_handedness])\n",
    "                    elif first_pipeline_gesture.category_name == 'Open_Palm':\n",
    "                        final_score = first_pipeline_score\n",
    "                        final_landmarks.append(landmarks_first_pipeline[first_pipeline_handedness])\n",
    "                    elif second_pipeline_gesture.category_name == 'Open_Palm':\n",
    "                        final_score = second_pipeline_score\n",
    "                        final_landmarks.append(landmarks_second_pipeline[second_pipeline_handedness])\n",
    "                    else:\n",
    "                        if first_pipeline_gesture.score < second_pipeline_gesture.score:\n",
    "                            final_score = first_pipeline_score\n",
    "                            final_landmarks.append(landmarks_first_pipeline[first_pipeline_handedness])\n",
    "                        else:\n",
    "                            final_score = second_pipeline_score\n",
    "                            final_landmarks.append(landmarks_second_pipeline[second_pipeline_handedness])\n",
    "                            \n",
    "            if len(recognizer_results_first_pipeline.gestures) == 0 :\n",
    "                final_score = second_pipeline_score\n",
    "                final_landmarks.append(landmarks_second_pipeline[\"Left\"])\n",
    "                final_landmarks.append(landmarks_second_pipeline[\"Right\"])\n",
    "                \n",
    "            if len(recognizer_results_second_pipeline.gestures) == 0:\n",
    "                final_score = first_pipeline_score\n",
    "                final_landmarks.append(landmarks_first_pipeline[\"Left\"])\n",
    "                final_landmarks.append(landmarks_first_pipeline[\"Right\"])\n",
    "                        \n",
    "                        \n",
    "            final_landmark_accuracy = calculate_hand_accuracies(ground_truth_landmarks, final_landmarks, [\"Left\", \"Right\"], distance_threshold)\n",
    "            final_landmark_accuracy[\"Left\"] = [0.0]*21 if np.isnan(np.mean(final_landmark_accuracy[\"Left\"])) else final_landmark_accuracy[\"Left\"]\n",
    "            final_landmark_accuracy[\"Right\"] = [0.0]*21 if np.isnan(np.mean(final_landmark_accuracy[\"Right\"])) else final_landmark_accuracy[\"Right\"]\n",
    "            \n",
    "            # Check if the final landmark accuracies and the final score are the same\n",
    "            # if final_landmark_accuracy[\"Left\"] != final_score[\"Left\"] or final_landmark_accuracy[\"Right\"] != final_score[\"Right\"]:\n",
    "            #     print(\"Final landmark accuracies and final score are not the same!!!!!!\")\n",
    "            #     print(f\"Final landmark accuracies - Left: {np.mean(final_landmark_accuracy['Left']):.2f}%, Right: {np.mean(final_landmark_accuracy['Right']):.2f}%\")\n",
    "            #     print(f\"Final score - Left: {np.mean(final_score['Left']):.2f}%, Right: {np.mean(final_score['Right']):.2f}%\")\n",
    "            \n",
    "            # Annotate with final landmarks\n",
    "            annotated_image_path = f\"../../resources/stylized-pictures/annotated/{image_name}_annotated.png\"\n",
    "            annotate_landmarks(image_path, final_landmarks, annotated_image_path)\n",
    "\n",
    "                        \n",
    "            # print(f\"Results for final score - Left: {np.mean(final_score['Left']):.2f}%, Right: {np.mean(final_score['Right']):.2f}%\")   \n",
    "            # print(f\"Length of final landmarks - {len(final_landmarks)}\")\n",
    "            print(\"---------------------------------------------------------\")\n",
    "\n",
    "          \n",
    "            for hand in [\"Left\", \"Right\"]:\n",
    "                total_transformed_siggraph17[hand].extend(final_landmark_accuracy[hand])\n",
    "\n"
   ],
   "id": "3071ab27f6f87b9c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image: 10_IMG20241127100740.jpg\n",
      "Results for original pipeline - Left: 82.19%, Right: 83.34%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/10_IMG20241127100740.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "---------------------------------------------------------\n",
      "Processing image: 11_IMG20241127100906.jpg\n",
      "Results for original pipeline - Left: 85.30%, Right: 79.65%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/11_IMG20241127100906.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "---------------------------------------------------------\n",
      "Processing image: 12_IMG20241127100913.jpg\n",
      "Results for original pipeline - Left: 80.91%, Right: 80.72%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/12_IMG20241127100913.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "---------------------------------------------------------\n",
      "Processing image: 13_IMG20241127101012.jpg\n",
      "Results for original pipeline - Left: 83.32%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/13_IMG20241127101012.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "COULD NOT ANNOTATE LANDMARKS FOR ../../RESOURCES/EVALUATION_DATASET/IR_ROTATED/13_IMG20241127101012.JPG: LANDMARK INDEX IS OUT OF RANGE. INVALID CONNECTION FROM LANDMARK #3 TO LANDMARK #4.\n",
      "---------------------------------------------------------\n",
      "Processing image: 14_IMG20241127101019.jpg\n",
      "Results for original pipeline - Left: 85.00%, Right: 71.96%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/14_IMG20241127101019.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "---------------------------------------------------------\n",
      "Processing image: 15_IMG20241127101422.jpg\n",
      "Results for original pipeline - Left: 83.29%, Right: 78.97%\n",
      "Results for second pipeline - Left: 82.21%, Right: 79.10%\n",
      "Both recognized open palm\n",
      "Both recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 16_IMG20241127101432.jpg\n",
      "Results for original pipeline - Left: 80.41%, Right: 78.99%\n",
      "Results for second pipeline - Left: 80.84%, Right: 0.00%\n",
      "---------------------------------------------------------\n",
      "Processing image: 17_IMG20241127101442.jpg\n",
      "Results for original pipeline - Left: 82.81%, Right: 78.99%\n",
      "Results for second pipeline - Left: 82.00%, Right: 63.82%\n",
      "Both recognized open palm\n",
      "Both recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 18_IMG20241127101447.jpg\n",
      "Results for original pipeline - Left: 80.71%, Right: 80.19%\n",
      "Results for second pipeline - Left: 81.51%, Right: 80.00%\n",
      "Both recognized open palm\n",
      "Both recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 19_IMG20241127101600.jpg\n",
      "Results for original pipeline - Left: 79.34%, Right: 80.32%\n",
      "Results for second pipeline - Left: 77.15%, Right: 78.84%\n",
      "Both recognized open palm\n",
      "Both recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 1_IMG20241127100244.jpg\n",
      "Results for original pipeline - Left: 27.92%, Right: 47.93%\n",
      "Results for second pipeline - Left: 83.29%, Right: 79.53%\n",
      "Both recognized open palm\n",
      "Both recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 20_IMG20241127101607.jpg\n",
      "Results for original pipeline - Left: 81.77%, Right: 81.03%\n",
      "Results for second pipeline - Left: 79.15%, Right: 78.92%\n",
      "Both recognized open palm\n",
      "Both recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 21_IMG20241127101704.jpg\n",
      "Results for original pipeline - Left: 75.10%, Right: 82.15%\n",
      "Results for second pipeline - Left: 75.21%, Right: 77.56%\n",
      "Both recognized open palm\n",
      "Both recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 22_IMG20241127101709.jpg\n",
      "Results for original pipeline - Left: 81.00%, Right: 79.60%\n",
      "Results for second pipeline - Left: 76.84%, Right: 78.36%\n",
      "Both recognized open palm\n",
      "Both recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 23_IMG20241127101749.jpg\n",
      "Results for original pipeline - Left: 58.73%, Right: 75.11%\n",
      "Results for second pipeline - Left: 71.38%, Right: 76.30%\n",
      "Both recognized open palm\n",
      "Both recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 24_IMG20241127101754.jpg\n",
      "Results for original pipeline - Left: 79.68%, Right: 76.68%\n",
      "Results for second pipeline - Left: 0.00%, Right: 77.74%\n",
      "---------------------------------------------------------\n",
      "Processing image: 25_IMG20241127101835.jpg\n",
      "Results for original pipeline - Left: 80.36%, Right: 84.68%\n",
      "Results for second pipeline - Left: 82.62%, Right: 82.58%\n",
      "Both recognized open palm\n",
      "Both recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 26_IMG20241127101840.jpg\n",
      "Results for original pipeline - Left: 83.95%, Right: 79.45%\n",
      "Results for second pipeline - Left: 81.03%, Right: 76.35%\n",
      "Both recognized open palm\n",
      "Both recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 27_IMG20241127101939.jpg\n",
      "Results for original pipeline - Left: 81.84%, Right: 81.17%\n",
      "Results for second pipeline - Left: 0.00%, Right: 82.03%\n",
      "---------------------------------------------------------\n",
      "Processing image: 28_IMG20241127101946.jpg\n",
      "Results for original pipeline - Left: 83.73%, Right: 83.89%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/28_IMG20241127101946.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "---------------------------------------------------------\n",
      "Processing image: 29_IMG20241127102051.jpg\n",
      "Results for original pipeline - Left: 0.00%, Right: 47.20%\n",
      "Results for second pipeline - Left: 62.37%, Right: 40.56%\n",
      "---------------------------------------------------------\n",
      "Processing image: 2_IMG20241127100252.jpg\n",
      "Results for original pipeline - Left: 39.82%, Right: 37.67%\n",
      "Results for second pipeline - Left: 77.77%, Right: 75.09%\n",
      "Second recognized open palm\n",
      "Both recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 30_IMG20241127102058.jpg\n",
      "No hands detected for ../../resources/stylized-pictures/siggraph17/30_IMG20241127102058.jpg_siggraph17.png\n",
      "Results for original pipeline - Left: 0.00%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/30_IMG20241127102058.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "COULD NOT ANNOTATE LANDMARKS FOR ../../RESOURCES/EVALUATION_DATASET/IR_ROTATED/30_IMG20241127102058.JPG: LANDMARK INDEX IS OUT OF RANGE. INVALID CONNECTION FROM LANDMARK #3 TO LANDMARK #4.\n",
      "---------------------------------------------------------\n",
      "Processing image: 31_IMG20241127102205.jpg\n",
      "Results for original pipeline - Left: 74.44%, Right: 76.47%\n",
      "Results for second pipeline - Left: 76.68%, Right: 78.92%\n",
      "Both recognized open palm\n",
      "Both recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 32_IMG20241127102216.jpg\n",
      "Results for original pipeline - Left: 74.59%, Right: 76.51%\n",
      "Results for second pipeline - Left: 77.20%, Right: 77.46%\n",
      "Both recognized open palm\n",
      "Both recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 33_IMG20241127102317.jpg\n",
      "Results for original pipeline - Left: 59.71%, Right: 59.53%\n",
      "Results for second pipeline - Left: 66.62%, Right: 66.55%\n",
      "Both recognized open palm\n",
      "Both recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 34_IMG20241127102423.jpg\n",
      "Results for original pipeline - Left: 66.45%, Right: 74.07%\n",
      "Results for second pipeline - Left: 72.10%, Right: 78.32%\n",
      "Second recognized open palm\n",
      "Both recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 35_IMG20241127102430.jpg\n",
      "Results for original pipeline - Left: 61.10%, Right: 65.76%\n",
      "Results for second pipeline - Left: 0.00%, Right: 71.61%\n",
      "---------------------------------------------------------\n",
      "Processing image: 36_IMG20241127102532.jpg\n",
      "Results for original pipeline - Left: 58.33%, Right: 48.28%\n",
      "Results for second pipeline - Left: 0.00%, Right: 78.88%\n",
      "---------------------------------------------------------\n",
      "Processing image: 37_IMG20241127102538.jpg\n",
      "Results for original pipeline - Left: 64.78%, Right: 44.17%\n",
      "Results for second pipeline - Left: 0.00%, Right: 75.62%\n",
      "---------------------------------------------------------\n",
      "Processing image: 38_IMG20241127102643.jpg\n",
      "Results for original pipeline - Left: 38.06%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/38_IMG20241127102643.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "COULD NOT ANNOTATE LANDMARKS FOR ../../RESOURCES/EVALUATION_DATASET/IR_ROTATED/38_IMG20241127102643.JPG: LANDMARK INDEX IS OUT OF RANGE. INVALID CONNECTION FROM LANDMARK #3 TO LANDMARK #4.\n",
      "---------------------------------------------------------\n",
      "Processing image: 39_IMG20241127102649.jpg\n",
      "No hands detected for ../../resources/stylized-pictures/siggraph17/39_IMG20241127102649.jpg_siggraph17.png\n",
      "Results for original pipeline - Left: 0.00%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/39_IMG20241127102649.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "COULD NOT ANNOTATE LANDMARKS FOR ../../RESOURCES/EVALUATION_DATASET/IR_ROTATED/39_IMG20241127102649.JPG: LANDMARK INDEX IS OUT OF RANGE. INVALID CONNECTION FROM LANDMARK #3 TO LANDMARK #4.\n",
      "---------------------------------------------------------\n",
      "Processing image: 3_IMG20241127100408.jpg\n",
      "Results for original pipeline - Left: 83.45%, Right: 52.12%\n",
      "Results for second pipeline - Left: 81.88%, Right: 0.00%\n",
      "COULD NOT ANNOTATE LANDMARKS FOR ../../RESOURCES/EVALUATION_DATASET/IR_ROTATED/3_IMG20241127100408.JPG: LANDMARK INDEX IS OUT OF RANGE. INVALID CONNECTION FROM LANDMARK #3 TO LANDMARK #4.\n",
      "---------------------------------------------------------\n",
      "Processing image: 40_IMG20241127102805.jpg\n",
      "No hands detected for ../../resources/stylized-pictures/siggraph17/40_IMG20241127102805.jpg_siggraph17.png\n",
      "Results for original pipeline - Left: 0.00%, Right: 0.00%\n",
      "Results for second pipeline - Left: 0.00%, Right: 84.42%\n",
      "COULD NOT ANNOTATE LANDMARKS FOR ../../RESOURCES/EVALUATION_DATASET/IR_ROTATED/40_IMG20241127102805.JPG: LANDMARK INDEX IS OUT OF RANGE. INVALID CONNECTION FROM LANDMARK #3 TO LANDMARK #4.\n",
      "---------------------------------------------------------\n",
      "Processing image: 41_IMG20241127102811.jpg\n",
      "Results for original pipeline - Left: 75.07%, Right: 0.00%\n",
      "Results for second pipeline - Left: 0.00%, Right: 41.44%\n",
      "---------------------------------------------------------\n",
      "Processing image: 42_IMG20241127102854.jpg\n",
      "Results for original pipeline - Left: 73.12%, Right: 74.93%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/42_IMG20241127102854.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "---------------------------------------------------------\n",
      "Processing image: 43_IMG20241127102901.jpg\n",
      "Results for original pipeline - Left: 81.42%, Right: 82.94%\n",
      "Results for second pipeline - Left: 77.73%, Right: 0.00%\n",
      "---------------------------------------------------------\n",
      "Processing image: 44_IMG20241127103107.jpg\n",
      "Results for original pipeline - Left: 82.10%, Right: 82.12%\n",
      "Results for second pipeline - Left: 82.28%, Right: 72.24%\n",
      "Both recognized open palm\n",
      "Both recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 45_IMG20241127103113.jpg\n",
      "Results for original pipeline - Left: 78.56%, Right: 80.90%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/45_IMG20241127103113.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "---------------------------------------------------------\n",
      "Processing image: 46_IMG20241127103125.jpg\n",
      "Results for original pipeline - Left: 77.78%, Right: 81.08%\n",
      "Results for second pipeline - Left: 0.00%, Right: 82.04%\n",
      "---------------------------------------------------------\n",
      "Processing image: 47_IMG20241127103130.jpg\n",
      "Results for original pipeline - Left: 84.11%, Right: 79.29%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/47_IMG20241127103130.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "---------------------------------------------------------\n",
      "Processing image: 48_IMG20241127103201.jpg\n",
      "Results for original pipeline - Left: 83.24%, Right: 78.33%\n",
      "Results for second pipeline - Left: 0.00%, Right: 80.08%\n",
      "---------------------------------------------------------\n",
      "Processing image: 49_IMG20241127103207.jpg\n",
      "Results for original pipeline - Left: 82.81%, Right: 76.55%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/49_IMG20241127103207.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "---------------------------------------------------------\n",
      "Processing image: 4_IMG20241127100414.jpg\n",
      "Results for original pipeline - Left: 83.16%, Right: 61.64%\n",
      "Results for second pipeline - Left: 0.00%, Right: 79.95%\n",
      "---------------------------------------------------------\n",
      "Processing image: 50_IMG20241127103243.jpg\n",
      "Results for original pipeline - Left: 82.67%, Right: 77.23%\n",
      "Results for second pipeline - Left: 0.00%, Right: 77.94%\n",
      "---------------------------------------------------------\n",
      "Processing image: 51_IMG20241127103250.jpg\n",
      "Results for original pipeline - Left: 81.68%, Right: 81.22%\n",
      "Results for second pipeline - Left: 0.00%, Right: 81.04%\n",
      "---------------------------------------------------------\n",
      "Processing image: 52_IMG20241127103332.jpg\n",
      "Results for original pipeline - Left: 72.88%, Right: 79.68%\n",
      "Results for second pipeline - Left: 77.64%, Right: 79.87%\n",
      "Both recognized open palm\n",
      "Both recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 53_IMG20241127103337.jpg\n",
      "Results for original pipeline - Left: 76.76%, Right: 78.55%\n",
      "Results for second pipeline - Left: 78.59%, Right: 78.25%\n",
      "Both recognized open palm\n",
      "Both recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 54_IMG20241127103409.jpg\n",
      "Results for original pipeline - Left: 63.58%, Right: 55.77%\n",
      "Results for second pipeline - Left: 67.32%, Right: 77.03%\n",
      "Second recognized open palm\n",
      "Neither recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 55_IMG20241127103413.jpg\n",
      "Results for original pipeline - Left: 67.99%, Right: 56.18%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/55_IMG20241127103413.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "---------------------------------------------------------\n",
      "Processing image: 56_IMG20241127103449.jpg\n",
      "Results for original pipeline - Left: 75.00%, Right: 40.03%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/56_IMG20241127103449.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "---------------------------------------------------------\n",
      "Processing image: 57_IMG20241127103455.jpg\n",
      "Results for original pipeline - Left: 0.00%, Right: 38.34%\n",
      "Results for second pipeline - Left: 61.96%, Right: 69.14%\n",
      "---------------------------------------------------------\n",
      "Processing image: 58_IMG20241127103532.jpg\n",
      "Results for original pipeline - Left: 76.95%, Right: 62.25%\n",
      "Results for second pipeline - Left: 72.35%, Right: 51.81%\n",
      "First recognized open palm\n",
      "Both recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 59_IMG20241127103541.jpg\n",
      "Results for original pipeline - Left: 28.27%, Right: 41.31%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/59_IMG20241127103541.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "---------------------------------------------------------\n",
      "Processing image: 5_IMG20241127100507.jpg\n",
      "Results for original pipeline - Left: 81.92%, Right: 80.70%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/5_IMG20241127100507.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "---------------------------------------------------------\n",
      "Processing image: 60_IMG20241127103613.jpg\n",
      "Results for original pipeline - Left: 79.29%, Right: 76.20%\n",
      "Results for second pipeline - Left: 80.86%, Right: 77.23%\n",
      "Both recognized open palm\n",
      "Both recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 61_IMG20241127103622.jpg\n",
      "Results for original pipeline - Left: 74.96%, Right: 79.30%\n",
      "Results for second pipeline - Left: 66.49%, Right: 77.89%\n",
      "Second recognized open palm\n",
      "Both recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 62_IMG20241127103640.jpg\n",
      "Results for original pipeline - Left: 70.35%, Right: 74.82%\n",
      "Results for second pipeline - Left: 77.19%, Right: 81.32%\n",
      "Both recognized open palm\n",
      "Both recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 63_IMG20241127103645.jpg\n",
      "Results for original pipeline - Left: 72.10%, Right: 73.17%\n",
      "Results for second pipeline - Left: 79.82%, Right: 82.21%\n",
      "Both recognized open palm\n",
      "Both recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 64_IMG20241127103711.jpg\n",
      "Results for original pipeline - Left: 77.68%, Right: 71.38%\n",
      "Results for second pipeline - Left: 68.19%, Right: 73.93%\n",
      "Both recognized open palm\n",
      "Both recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 65_IMG20241127103717.jpg\n",
      "Results for original pipeline - Left: 79.49%, Right: 69.81%\n",
      "Results for second pipeline - Left: 0.00%, Right: 76.78%\n",
      "---------------------------------------------------------\n",
      "Processing image: 66_IMG20241127103745.jpg\n",
      "Results for original pipeline - Left: 53.10%, Right: 69.24%\n",
      "Results for second pipeline - Left: 65.72%, Right: 71.12%\n",
      "Both recognized open palm\n",
      "Second recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 67_IMG20241127103751.jpg\n",
      "Results for original pipeline - Left: 66.50%, Right: 74.04%\n",
      "Results for second pipeline - Left: 76.51%, Right: 70.92%\n",
      "Both recognized open palm\n",
      "Both recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 68_IMG20241127103819.jpg\n",
      "Results for original pipeline - Left: 39.84%, Right: 57.54%\n",
      "Results for second pipeline - Left: 56.36%, Right: 73.91%\n",
      "Second recognized open palm\n",
      "Second recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 69_IMG20241127103825.jpg\n",
      "Results for original pipeline - Left: 50.11%, Right: 67.42%\n",
      "Results for second pipeline - Left: 69.98%, Right: 67.47%\n",
      "Both recognized open palm\n",
      "Both recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 6_IMG20241127100515.jpg\n",
      "Results for original pipeline - Left: 74.90%, Right: 27.91%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/6_IMG20241127100515.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "---------------------------------------------------------\n",
      "Processing image: 70_IMG20241127103904.jpg\n",
      "Results for original pipeline - Left: 23.55%, Right: 37.47%\n",
      "Results for second pipeline - Left: 0.00%, Right: 41.13%\n",
      "---------------------------------------------------------\n",
      "Processing image: 71_IMG20241127103908.jpg\n",
      "Results for original pipeline - Left: 9.20%, Right: 45.83%\n",
      "Results for second pipeline - Left: 46.70%, Right: 63.48%\n",
      "Second recognized open palm\n",
      "Second recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 72_IMG20241127103941.jpg\n",
      "Results for original pipeline - Left: 2.35%, Right: 5.30%\n",
      "Results for second pipeline - Left: 55.26%, Right: 0.00%\n",
      "---------------------------------------------------------\n",
      "Processing image: 73_IMG20241127103945.jpg\n",
      "Results for original pipeline - Left: 7.30%, Right: 9.99%\n",
      "Results for second pipeline - Left: 0.00%, Right: 53.99%\n",
      "---------------------------------------------------------\n",
      "Processing image: 74_IMG20241127104014.jpg\n",
      "No hands detected for ../../resources/stylized-pictures/siggraph17/74_IMG20241127104014.jpg_siggraph17.png\n",
      "Results for original pipeline - Left: 0.00%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/74_IMG20241127104014.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "COULD NOT ANNOTATE LANDMARKS FOR ../../RESOURCES/EVALUATION_DATASET/IR_ROTATED/74_IMG20241127104014.JPG: LANDMARK INDEX IS OUT OF RANGE. INVALID CONNECTION FROM LANDMARK #3 TO LANDMARK #4.\n",
      "---------------------------------------------------------\n",
      "Processing image: 75_IMG20241127104020.jpg\n",
      "No hands detected for ../../resources/stylized-pictures/siggraph17/75_IMG20241127104020.jpg_siggraph17.png\n",
      "Results for original pipeline - Left: 0.00%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/75_IMG20241127104020.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "COULD NOT ANNOTATE LANDMARKS FOR ../../RESOURCES/EVALUATION_DATASET/IR_ROTATED/75_IMG20241127104020.JPG: LANDMARK INDEX IS OUT OF RANGE. INVALID CONNECTION FROM LANDMARK #3 TO LANDMARK #4.\n",
      "---------------------------------------------------------\n",
      "Processing image: 76_IMG20241127104628.jpg\n",
      "Results for original pipeline - Left: 82.18%, Right: 78.49%\n",
      "Results for second pipeline - Left: 84.32%, Right: 82.60%\n",
      "Both recognized open palm\n",
      "Both recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 77_IMG20241127104637.jpg\n",
      "Results for original pipeline - Left: 79.87%, Right: 81.16%\n",
      "Results for second pipeline - Left: 69.40%, Right: 80.21%\n",
      "Both recognized open palm\n",
      "Both recognized open palm\n",
      "---------------------------------------------------------\n",
      "Processing image: 78_IMG20241127105250.jpg\n",
      "Results for original pipeline - Left: 0.00%, Right: 71.09%\n",
      "Results for second pipeline - Left: 24.52%, Right: 67.32%\n",
      "---------------------------------------------------------\n",
      "Processing image: 79_IMG20241127105256.jpg\n",
      "Results for original pipeline - Left: 72.70%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/79_IMG20241127105256.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "COULD NOT ANNOTATE LANDMARKS FOR ../../RESOURCES/EVALUATION_DATASET/IR_ROTATED/79_IMG20241127105256.JPG: LANDMARK INDEX IS OUT OF RANGE. INVALID CONNECTION FROM LANDMARK #3 TO LANDMARK #4.\n",
      "---------------------------------------------------------\n",
      "Processing image: 7_IMG20241127100610.jpg\n",
      "Results for original pipeline - Left: 79.50%, Right: 82.79%\n",
      "Results for second pipeline - Left: 0.00%, Right: 64.05%\n",
      "---------------------------------------------------------\n",
      "Processing image: 80_IMG20241127105346.jpg\n",
      "Results for original pipeline - Left: 21.43%, Right: 68.15%\n",
      "Results for second pipeline - Left: 0.00%, Right: 79.01%\n",
      "---------------------------------------------------------\n",
      "Processing image: 8_IMG20241127100617.jpg\n",
      "Results for original pipeline - Left: 81.99%, Right: 41.75%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/8_IMG20241127100617.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "---------------------------------------------------------\n",
      "Processing image: 9_IMG20241127100730.jpg\n",
      "Results for original pipeline - Left: 79.32%, Right: 74.66%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/9_IMG20241127100730.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 188
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T13:44:07.722116Z",
     "start_time": "2024-12-29T13:44:07.706334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transformed_accuracy_siggraph17 = calculate_final_accuracy(total_transformed_siggraph17)\n",
    "# transformed_accuracy_eccv16 = calculate_final_accuracy(total_transformed_eccv16)"
   ],
   "id": "e65a45a34689ca5a",
   "outputs": [],
   "execution_count": 189
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T13:44:07.767165Z",
     "start_time": "2024-12-29T13:44:07.748280Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Transformed Accuracy (Siggraph17): {transformed_accuracy_siggraph17:.2f}%\")",
   "id": "d035ac7d6a6a6810",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Accuracy (Siggraph17): 65.30%\n"
     ]
    }
   ],
   "execution_count": 190
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T13:55:57.078276Z",
     "start_time": "2024-12-29T13:55:57.051786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def advanced_enhancement(image_path, image_name):\n",
    "    no_boxes_image_path = remove_temperature_boxes(image_path, image_name)\n",
    "    no_cross_path = remove_cross(no_boxes_image_path, image_name)\n",
    "    # Load the image in grayscale\n",
    "    image = cv2.imread(no_cross_path, cv2.IMREAD_GRAYSCALE)\n",
    "    # Apply histogram equalization\n",
    "    equalized = cv2.equalizeHist(image)\n",
    "    # Invert the colors\n",
    "    inverted = cv2.bitwise_not(equalized)\n",
    "    # Apply GaussianBlur to reduce noise and enhance features\n",
    "    blurred = cv2.GaussianBlur(inverted, (5, 5), 0)\n",
    "    # Combine equalization and inversion for better contrast\n",
    "    combined = cv2.addWeighted(equalized, 0.7, blurred, 0.3, 0)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    enhanced_image = clahe.apply(combined)\n",
    "    \n",
    "    # Change the image to RGB format\n",
    "    # sharpened_image = cv2.cvtColor(sharpened_image, cv2.COLOR_GRAY2RGB)\n",
    "    # Save the enhanced image\n",
    "    enhanced_path = f\"../../resources/stylized-pictures/enhanced/{image_name}_enhanced.png\"\n",
    "    cv2.imwrite(enhanced_path, cv2.cvtColor(enhanced_image, cv2.COLOR_GRAY2RGB))\n",
    "\n",
    "    return enhanced_path"
   ],
   "id": "a78646c5bf10bb49",
   "outputs": [],
   "execution_count": 222
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T13:59:51.785411Z",
     "start_time": "2024-12-29T13:59:48.014573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "image_name = '71_IMG20241127103908.jpg'\n",
    "image_path = f'../../resources/evaluation_dataset/IR_rotated/{image_name}'\n",
    "\n",
    "image_pipeline_one = execute_transformation_pipeline(image_path, image_name)\n",
    "image_pipeline_two = execute_transformation_pipeline_with_no_visible_fingers(image_path, image_name)\n",
    "image_advanced_enhancement = advanced_enhancement(image_path, image_name)\n",
    "\n",
    "with open(json_file_path, 'r') as f:\n",
    "    ground_truth_data = json.load(f)\n",
    "            \n",
    "# Find the entry where image value equals image_name\n",
    "matching_entry = next((entry for entry in ground_truth_data if entry[\"image\"] == image_name), None)\n",
    "ground_truth_landmarks = matching_entry[\"landmarks\"]\n",
    "\n",
    "first_pipeline_score = assess_accuracy(image_pipeline_one, ground_truth_landmarks)\n",
    "second_pipeline_score = assess_accuracy(image_pipeline_two, ground_truth_landmarks)\n",
    "advanced_enhancement_score = assess_accuracy(image_advanced_enhancement, ground_truth_landmarks)\n",
    "\n",
    "print(f\"Results for original pipeline - Left: {np.mean(first_pipeline_score['Left']):.2f}%, Right: {np.mean(first_pipeline_score['Right']):.2f}%\")\n",
    "print(f\"Results for second pipeline - Left: {np.mean(second_pipeline_score['Left']):.2f}%, Right: {np.mean(second_pipeline_score['Right']):.2f}%\")\n",
    "print(f\"Results for advanced enhancement - Left: {np.mean(advanced_enhancement_score['Left']):.2f}%, Right: {np.mean(advanced_enhancement_score['Right']):.2f}%\")\n",
    "\n"
   ],
   "id": "912a381c53005bd9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for original pipeline - Left: 9.20%, Right: 45.83%\n",
      "Results for second pipeline - Left: 46.70%, Right: 63.48%\n",
      "Results for advanced enhancement - Left: 55.55%, Right: 67.36%\n"
     ]
    }
   ],
   "execution_count": 226
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T13:51:57.038053Z",
     "start_time": "2024-12-29T13:51:57.022221Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "1b03d111261cf5ea",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
