{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:01.226004Z",
     "start_time": "2025-01-06T11:16:01.209942Z"
    }
   },
   "source": [
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "from mediapipe.tasks.python.components.containers import NormalizedLandmark\n",
    "from rembg import remove\n",
    "from PIL import Image\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Parameters",
   "id": "1a49b1954276f7f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:01.473342Z",
     "start_time": "2025-01-06T11:16:01.444692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mediapipe_model_path = '../hand_landmarker.task'\n",
    "mediapipe_gesture_recognizer_model_path = '../gesture_recognizer.task'\n",
    "RGB_image_directory = '../../resources/evaluation_dataset/RGB'\n",
    "RGB_ground_truth_directory = '../../resources/evaluation_dataset/RGB_annotation'\n",
    "IR_image_directory = '../../resources/evaluation_dataset/IR'\n",
    "IR_ground_truth_directory = '../../resources/evaluation_dataset/IR_annotations'\n",
    "bad_score_penalty = 1\n",
    "distance_threshold = 0.05"
   ],
   "id": "3489d579e81a5cc9",
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create Models",
   "id": "9ee8b2ca26ced1a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:01.856424Z",
     "start_time": "2025-01-06T11:16:01.769439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_options = python.BaseOptions(model_asset_path=mediapipe_gesture_recognizer_model_path)\n",
    "options = vision.GestureRecognizerOptions(base_options=base_options,\n",
    "                                          num_hands=2)\n",
    "\n",
    "options.canned_gesture_classifier_options.score_threshold = 0\n",
    "gesture_recognizer = vision.GestureRecognizer.create_from_options(options)"
   ],
   "id": "5b272bc566577fe5",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:01.987833Z",
     "start_time": "2025-01-06T11:16:01.972199Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "91d5d87de4166488",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Drawing Landmarks",
   "id": "10fd4b7d9e0fb4f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:02.520767Z",
     "start_time": "2025-01-06T11:16:02.505171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MARGIN = 10  # pixels\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "HANDEDNESS_TEXT_COLOR = (88, 205, 54) # vibrant green\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "  hand_landmarks_list = detection_result.hand_landmarks\n",
    "  handedness_list = detection_result.handedness\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  # Loop through the detected hands to visualize.\n",
    "  for idx in range(len(hand_landmarks_list)):\n",
    "    hand_landmarks = hand_landmarks_list[idx]\n",
    "    handedness = handedness_list[idx]\n",
    "\n",
    "    # Draw the hand landmarks.\n",
    "    hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    hand_landmarks_proto.landmark.extend([\n",
    "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
    "    ])\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "      annotated_image,\n",
    "      hand_landmarks_proto,\n",
    "      solutions.hands.HAND_CONNECTIONS,\n",
    "      solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "      solutions.drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "    # Get the top left corner of the detected hand's bounding box.\n",
    "    height, width, _ = annotated_image.shape\n",
    "    x_coordinates = [landmark.x for landmark in hand_landmarks]\n",
    "    y_coordinates = [landmark.y for landmark in hand_landmarks]\n",
    "    text_x = int(min(x_coordinates) * width)\n",
    "    text_y = int(min(y_coordinates) * height) - MARGIN\n",
    "\n",
    "    # Draw handedness (left or right hand) on the image.\n",
    "    cv2.putText(annotated_image, f\"{handedness[0].category_name}\",\n",
    "                (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
    "                FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n",
    "\n",
    "  return annotated_image"
   ],
   "id": "2427619c14560a50",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:02.833212Z",
     "start_time": "2025-01-06T11:16:02.801530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MARGIN = 10  # pixels\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "HANDEDNESS_TEXT_COLOR = (88, 205, 54) # vibrant green\n",
    "\n",
    "def draw_landmarks_on_image_from_json(rgb_image, evaluation_result):\n",
    "  hand_landmarks_list = evaluation_result['landmarks']\n",
    "  # handedness_list = detection_result.handedness\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  # Loop through the detected hands to visualize.\n",
    "  for idx in range(len(hand_landmarks_list)):\n",
    "    hand_landmarks = hand_landmarks_list[idx]\n",
    "    # handedness = handedness_list[idx]\n",
    "    height, width, _ = annotated_image.shape\n",
    "    # Draw the hand landmarks.\n",
    "    hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    hand_landmarks_proto.landmark.extend([\n",
    "      landmark_pb2.NormalizedLandmark(x=float(landmark['x']), y=float(landmark['y']), z=0.0) for landmark in hand_landmarks\n",
    "    ])\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "      annotated_image,\n",
    "      hand_landmarks_proto,\n",
    "      solutions.hands.HAND_CONNECTIONS,\n",
    "      solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "      solutions.drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "    # Get the top left corner of the detected hand's bounding box.\n",
    "   \n",
    "    x_coordinates = [landmark['x'] for landmark in hand_landmarks]\n",
    "    y_coordinates = [landmark['y'] for landmark in hand_landmarks]\n",
    "    text_x = int(min(x_coordinates) * width)\n",
    "    text_y = int(min(y_coordinates) * height) - MARGIN\n",
    "\n",
    "    # Draw handedness (left or right hand) on the image.\n",
    "    cv2.putText(annotated_image, f\"Left\",\n",
    "                (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
    "                FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n",
    "\n",
    "  return annotated_image"
   ],
   "id": "8c59b8f6478c6ff9",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Assessing the Accuracy",
   "id": "a89c5474010875a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:03.213365Z",
     "start_time": "2025-01-06T11:16:03.206040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_accuracy(ground_truth, prediction, threshold):\n",
    "    if prediction is None:  # Missing landmark\n",
    "        return 0.0\n",
    "    distance = np.sqrt((float(ground_truth[\"x\"]) - prediction.x) ** 2 + \n",
    "                       (float(ground_truth[\"y\"]) - prediction.y) ** 2)\n",
    "    return max(0, 1 - distance / threshold) * 100"
   ],
   "id": "25bfd4e1071d2f8f",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:03.494533Z",
     "start_time": "2025-01-06T11:16:03.478909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from itertools import permutations\n",
    "\n",
    "def calculate_hand_accuracies(ground_truth_hands, predicted_hands, handedness_info, threshold):\n",
    "    accuracies = {\"Left\": [], \"Right\": []}\n",
    "\n",
    "    # If no hands detected, accuracy is 0% for both hands\n",
    "    if len(predicted_hands) == 0:\n",
    "        return {\"Left\": [0.0]*21, \"Right\": [0.0]*21}\n",
    "    \n",
    "\n",
    "    first_predicted_hand = predicted_hands[0]\n",
    "\n",
    "    hand_accuracy_1 = [\n",
    "        calculate_accuracy(gt_landmark, pred_landmark, threshold)\n",
    "        for gt_landmark, pred_landmark in zip(ground_truth_hands[0], first_predicted_hand)\n",
    "    ]\n",
    "\n",
    "    hand_accuracy_2 = [\n",
    "        calculate_accuracy(gt_landmark, pred_landmark, threshold)\n",
    "        for gt_landmark, pred_landmark in zip(ground_truth_hands[1], first_predicted_hand)\n",
    "    ]\n",
    "\n",
    "    if len(predicted_hands) == 1 and sum(hand_accuracy_1) > sum(hand_accuracy_2):\n",
    "        accuracies[\"Left\"] = hand_accuracy_1\n",
    "        accuracies[\"Right\"] = [0.0]*21\n",
    "        return accuracies\n",
    "    elif len(predicted_hands) == 1:\n",
    "        accuracies[\"Right\"] = hand_accuracy_2\n",
    "        accuracies[\"Left\"] = [0.0]*21\n",
    "        return accuracies\n",
    "    else:\n",
    "        second_predicted_hand = predicted_hands[1]\n",
    "        hand_accuracy_3 = [\n",
    "            calculate_accuracy(gt_landmark, pred_landmark, threshold)\n",
    "            for gt_landmark, pred_landmark in zip(ground_truth_hands[0], second_predicted_hand)\n",
    "        ]\n",
    "\n",
    "        hand_accuracy_4 = [\n",
    "            calculate_accuracy(gt_landmark, pred_landmark, threshold)\n",
    "            for gt_landmark, pred_landmark in zip(ground_truth_hands[1], second_predicted_hand)\n",
    "        ]\n",
    "\n",
    "        if sum(hand_accuracy_1) + sum(hand_accuracy_4) > sum(hand_accuracy_2) + sum(hand_accuracy_3):\n",
    "            accuracies[\"Left\"] = hand_accuracy_1\n",
    "            accuracies[\"Right\"] = hand_accuracy_4\n",
    "        else:\n",
    "            accuracies[\"Right\"] = hand_accuracy_2\n",
    "            accuracies[\"Left\"] = hand_accuracy_3\n",
    "\n",
    "    return accuracies"
   ],
   "id": "3e1e1b9685c123c5",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:03.752757Z",
     "start_time": "2025-01-06T11:16:03.737023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def assess_accuracy(image_path, ground_truth_hands):\n",
    "    \"\"\"\n",
    "    Assess accuracy of predictions for a given image.\n",
    "    \"\"\"\n",
    "    image = mp.Image.create_from_file(image_path)\n",
    "    \n",
    "    results = gesture_recognizer.recognize(image)\n",
    "    \n",
    "    if not results.hand_landmarks or len(results.hand_landmarks) == 0:\n",
    "        print(f\"No hands detected for {image_path}\")\n",
    "        return {\"Left\": [0.0]*21, \"Right\": [0.0]*21}\n",
    "\n",
    "    return calculate_hand_accuracies(\n",
    "        ground_truth_hands,\n",
    "        results.hand_landmarks,\n",
    "        results.handedness,\n",
    "        distance_threshold\n",
    "    )"
   ],
   "id": "ba74de54791fbfe1",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Calculating Distance and Average Sum of Distance for the Dataset",
   "id": "3b2cbb064e13dc48"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:04.236853Z",
     "start_time": "2025-01-06T11:16:04.205278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_euclidean_distance(ground_truth, prediction):\n",
    "    distance = np.sqrt((float(ground_truth[\"x\"]) - prediction.x) ** 2 + \n",
    "                       (float(ground_truth[\"y\"]) - prediction.y) ** 2)\n",
    "    return distance"
   ],
   "id": "bd037d6f1070e035",
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:04.518529Z",
     "start_time": "2025-01-06T11:16:04.486873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_sum_of_distances(ground_truth_hands, predicted_hands):\n",
    "    distances = {\"Left\": [], \"Right\": []}\n",
    "    if len(predicted_hands) == 0:\n",
    "        return sum([bad_score_penalty*21]) + sum([bad_score_penalty*21])\n",
    "    \n",
    "    if len(predicted_hands) == 1:\n",
    "        print(\"Only one hand detected\")\n",
    "\n",
    "    first_predicted_hand = predicted_hands[0]\n",
    "\n",
    "    hand_distances_1 = [\n",
    "        calculate_euclidean_distance(gt_landmark, pred_landmark)\n",
    "        for gt_landmark, pred_landmark in zip(ground_truth_hands[0], first_predicted_hand)\n",
    "    ]\n",
    "\n",
    "    hand_distances_2 = [\n",
    "        calculate_euclidean_distance(gt_landmark, pred_landmark)\n",
    "        for gt_landmark, pred_landmark in zip(ground_truth_hands[1], first_predicted_hand)\n",
    "    ]\n",
    "\n",
    "    if len(predicted_hands) == 1 and sum(hand_distances_1) < sum(hand_distances_2):\n",
    "        distances[\"Left\"] = sum(hand_distances_1)\n",
    "        distances[\"Right\"] = sum([bad_score_penalty*21])\n",
    "    elif len(predicted_hands) == 1:\n",
    "        distances[\"Right\"] = sum(hand_distances_2)\n",
    "        distances[\"Left\"] = sum([bad_score_penalty*21])\n",
    "    else:\n",
    "        second_predicted_hand = predicted_hands[1]\n",
    "        hand_distances_3 = [\n",
    "            calculate_euclidean_distance(gt_landmark, pred_landmark)\n",
    "            for gt_landmark, pred_landmark in zip(ground_truth_hands[0], second_predicted_hand)\n",
    "        ]\n",
    "\n",
    "        hand_distances_4 = [\n",
    "            calculate_euclidean_distance(gt_landmark, pred_landmark)\n",
    "            for gt_landmark, pred_landmark in zip(ground_truth_hands[1], second_predicted_hand)\n",
    "        ]\n",
    "\n",
    "        if sum(hand_distances_1) + sum(hand_distances_4) < sum(hand_distances_2) + sum(hand_distances_3):\n",
    "            distances[\"Left\"] = sum(hand_distances_1)\n",
    "            distances[\"Right\"] = sum(hand_distances_4)\n",
    "        else:\n",
    "            distances[\"Right\"] = sum(hand_distances_2)\n",
    "            distances[\"Left\"] = sum(hand_distances_3)\n",
    "            \n",
    "    \n",
    "\n",
    "    return distances[\"Left\"] + distances[\"Right\"]"
   ],
   "id": "6a0b299e0819002c",
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:04.768734Z",
     "start_time": "2025-01-06T11:16:04.753118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_sum_of_distances_for_image(image_path, ground_truth_hands):\n",
    "    image = mp.Image.create_from_file(image_path)\n",
    "    results = gesture_recognizer.recognize(image)\n",
    "    distance = calculate_sum_of_distances(ground_truth_hands, results.hand_landmarks)\n",
    "    \n",
    "    return distance"
   ],
   "id": "9b52478ced6d1e4f",
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:05.012370Z",
     "start_time": "2025-01-06T11:16:04.996746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_distances(ground_truth_hands, predicted_hands):\n",
    "    distances = {\"Left\": [], \"Right\": []}\n",
    "    if len(predicted_hands) == 0:\n",
    "        return {\"Left\": [bad_score_penalty] * 21, \"Right\": [bad_score_penalty] * 21 }\n",
    "    \n",
    "    if len(predicted_hands) == 1:\n",
    "        print(\"Only one hand detected\")\n",
    "\n",
    "    first_predicted_hand = predicted_hands[0]\n",
    "\n",
    "    hand_distances_1 = [\n",
    "        calculate_euclidean_distance(gt_landmark, pred_landmark)\n",
    "        for gt_landmark, pred_landmark in zip(ground_truth_hands[0], first_predicted_hand)\n",
    "    ]\n",
    "\n",
    "    hand_distances_2 = [\n",
    "        calculate_euclidean_distance(gt_landmark, pred_landmark)\n",
    "        for gt_landmark, pred_landmark in zip(ground_truth_hands[1], first_predicted_hand)\n",
    "    ]\n",
    "\n",
    "    if len(predicted_hands) == 1 and sum(hand_distances_1) < sum(hand_distances_2):\n",
    "        distances[\"Left\"] = hand_distances_1\n",
    "        distances[\"Right\"] = [bad_score_penalty] * 21\n",
    "    elif len(predicted_hands) == 1:\n",
    "        distances[\"Right\"] = hand_distances_2\n",
    "        distances[\"Left\"] = [bad_score_penalty] * 21\n",
    "    else:\n",
    "        second_predicted_hand = predicted_hands[1]\n",
    "        hand_distances_3 = [\n",
    "            calculate_euclidean_distance(gt_landmark, pred_landmark)\n",
    "            for gt_landmark, pred_landmark in zip(ground_truth_hands[0], second_predicted_hand)\n",
    "        ]\n",
    "\n",
    "        hand_distances_4 = [\n",
    "            calculate_euclidean_distance(gt_landmark, pred_landmark)\n",
    "            for gt_landmark, pred_landmark in zip(ground_truth_hands[1], second_predicted_hand)\n",
    "        ]\n",
    "\n",
    "        if sum(hand_distances_1) + sum(hand_distances_4) < sum(hand_distances_2) + sum(hand_distances_3):\n",
    "            distances[\"Left\"] = hand_distances_1\n",
    "            distances[\"Right\"] = hand_distances_4\n",
    "        else:\n",
    "            distances[\"Right\"] = hand_distances_2\n",
    "            distances[\"Left\"] = hand_distances_3\n",
    "            \n",
    "    return distances"
   ],
   "id": "232c3da9a1202f3a",
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:05.316400Z",
     "start_time": "2025-01-06T11:16:05.300719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_avg_distance_for_image(image_path, ground_truth_hands):\n",
    "    image = mp.Image.create_from_file(image_path)\n",
    "    results = gesture_recognizer.recognize(image)\n",
    "    distances = calculate_distances(ground_truth_hands, results.hand_landmarks)\n",
    "    \n",
    "    return sum(distances[\"Left\"] + distances[\"Right\"]) / len(distances[\"Left\"] + distances[\"Right\"])"
   ],
   "id": "17371f80dc4a2861",
   "outputs": [],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:05.560216Z",
     "start_time": "2025-01-06T11:16:05.544532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_distances_for_image(image_path, ground_truth_hands):\n",
    "    image = mp.Image.create_from_file(image_path)\n",
    "    results = gesture_recognizer.recognize(image)\n",
    "    distances = calculate_distances(ground_truth_hands, results.hand_landmarks)\n",
    "    \n",
    "    return distances[\"Left\"] + distances[\"Right\"]"
   ],
   "id": "f5e1728539b74ead",
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating Thresholds/Bounds\n",
   "id": "238d1ae0f27a83a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:06.081348Z",
     "start_time": "2025-01-06T11:16:06.049651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_final_accuracy(scores):\n",
    "    all_scores = scores[\"Left\"] + scores[\"Right\"]\n",
    "    return sum(all_scores) / len(all_scores) if all_scores else 0.0"
   ],
   "id": "cb280ec4155ddba6",
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:06.382435Z",
     "start_time": "2025-01-06T11:16:06.351176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def calculate_bound_accuracy(image_directory, ground_truth_directory):\n",
    "    total_bound = {\"Left\": [], \"Right\":[]}\n",
    "    for file_name in os.listdir(ground_truth_directory):\n",
    "        if file_name.endswith('.json'):\n",
    "            json_file_path = os.path.join(ground_truth_directory, file_name)\n",
    "            \n",
    "            with open(json_file_path, 'r') as f:\n",
    "                ground_truth_data = json.load(f)\n",
    "                \n",
    "            for entry in ground_truth_data:\n",
    "                image_name = entry[\"image\"]\n",
    "                ground_truth_landmarks = entry[\"landmarks\"]\n",
    "                image_path = os.path.join(image_directory, image_name)\n",
    "    \n",
    "                # Assess accuracy for RGB images (upper bound)\n",
    "                score = assess_accuracy(image_path, ground_truth_landmarks)\n",
    "                \n",
    "                for hand in [\"Left\", \"Right\"]:\n",
    "                    total_bound[hand].extend(score[hand])\n",
    "    return calculate_final_accuracy(total_bound)\n",
    "    "
   ],
   "id": "fd1d97cdedc6a2c5",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:20.762537Z",
     "start_time": "2025-01-06T11:16:06.651165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "upper_bound_accuracy = calculate_bound_accuracy(RGB_image_directory, RGB_ground_truth_directory)\n",
    "lower_bound_accuracy = calculate_bound_accuracy(IR_image_directory ,IR_ground_truth_directory)"
   ],
   "id": "53d9cc0ef275495a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hands detected for ../../resources/evaluation_dataset/IR\\3_IMG20241127100408.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR\\5_IMG20241127100507.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR\\6_IMG20241127100515.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR\\10_IMG20241127100740.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR\\11_IMG20241127100906.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR\\12_IMG20241127100913.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR\\13_IMG20241127101012.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR\\14_IMG20241127101019.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR\\29_IMG20241127102051.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR\\38_IMG20241127102643.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR\\39_IMG20241127102649.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR\\40_IMG20241127102805.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR\\41_IMG20241127102811.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR\\42_IMG20241127102854.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR\\45_IMG20241127103113.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR\\49_IMG20241127103207.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR\\57_IMG20241127103455.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR\\59_IMG20241127103541.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR\\75_IMG20241127104020.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR\\75_IMG20241127104020.jpg\n",
      "No hands detected for ../../resources/evaluation_dataset/IR\\79_IMG20241127105256.jpg\n"
     ]
    }
   ],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:20.793857Z",
     "start_time": "2025-01-06T11:16:20.762537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Upper Bound Accuracy (RGB): {upper_bound_accuracy:.2f}%\")\n",
    "print(f\"Lower Bound Accuracy (Infrared): {lower_bound_accuracy:.2f}%\")"
   ],
   "id": "68bfde699ca4b2af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upper Bound Accuracy (RGB): 84.26%\n",
      "Lower Bound Accuracy (Infrared): 42.29%\n"
     ]
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualizing Thresholds",
   "id": "5a02e8544a65e6d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:20.825047Z",
     "start_time": "2025-01-06T11:16:20.809422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def visualize_threshold_roi(image_path, ground_truth_landmarks, threshold, output_path):\n",
    "    \"\"\"\n",
    "    Visualizes regions of interest (ROIs) based on the threshold for accuracy.\n",
    "\n",
    "    Parameters:\n",
    "        image_path (str): Path to the image file.\n",
    "        ground_truth_landmarks (list of dict): Ground truth landmarks with 'x' and 'y' coordinates.\n",
    "        threshold (float): Distance threshold for accuracy calculation.\n",
    "        output_path (str): Path to save the output visualization image.\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    overlay = image.copy()\n",
    "\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    # Draw circles around the ground truth landmarks\n",
    "    for hand in ground_truth_landmarks:\n",
    "        for landmark in hand:\n",
    "            center = (int(float(landmark['x']) * width), int(float(landmark['y']) * height))\n",
    "            radius = int(threshold*width)  # Scale threshold to image dimensions\n",
    "            color = (0, 255, 0)  # Green color for the circle\n",
    "            alpha = 0.4  # Transparency factor\n",
    "    \n",
    "            # Draw the circle on the overlay\n",
    "            cv2.circle(overlay, center, radius, color, -1)\n",
    "\n",
    "    # Blend the overlay with the original image\n",
    "    cv2.addWeighted(overlay, alpha, image, 1 - alpha, 0, image)\n",
    "\n",
    "    # Save the output image\n",
    "    cv2.imwrite(output_path, image)"
   ],
   "id": "8d34912e9f7d4ff0",
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Transforming Images",
   "id": "27b48a742b6daddb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:20.857555Z",
     "start_time": "2025-01-06T11:16:20.841928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../colorization/zhang')"
   ],
   "id": "beffe87da273403a",
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:20.889211Z",
     "start_time": "2025-01-06T11:16:20.873575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.colorization.Zhang import create_colorized_pictures\n",
    "def colorize_zhang_eccv16(image_path, image_name):\n",
    "    create_colorized_pictures(model='eccv16', img_path=image_path, save_prefix=image_name)  \n",
    "    return f'../../resources/stylized-pictures/eccv16/{image_name}_eccv16.png'"
   ],
   "id": "64d3a755aeb977dc",
   "outputs": [],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:20.920473Z",
     "start_time": "2025-01-06T11:16:20.904839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def colorize_zhang_siggraph17(image_path, image_name):\n",
    "    create_colorized_pictures(model='siggraph17', img_path=image_path, save_prefix=image_name)  \n",
    "    return f'../../resources/stylized-pictures/siggraph17/{image_name}_siggraph17.png'\n",
    "    "
   ],
   "id": "fbd9306a4ef46991",
   "outputs": [],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:20.951726Z",
     "start_time": "2025-01-06T11:16:20.936103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clahe(image, clipLimit=2.0, tileGridSize=(8, 8), save_path=None, save_as_rgb=True):\n",
    "    \"\"\"\n",
    "    Applies Contrast Limited Adaptive Histogram Equalization (CLAHE) to an image.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: Input image (grayscale or BGR/RGB).\n",
    "    - clipLimit: Threshold for contrast limiting.\n",
    "    - tileGridSize: Size of the grid for histogram equalization.\n",
    "    - save_path: Path to save the output image.\n",
    "    - save_as_rgb: Save the CLAHE-applied image in RGB format if True.\n",
    "    \n",
    "    Returns:\n",
    "    - Processed image in grayscale or RGB format.\n",
    "    \"\"\"\n",
    "    # Convert to grayscale if it's a color image\n",
    "    if len(image.shape) == 3:  # If RGB/BGR\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = image\n",
    "\n",
    "    # Apply CLAHE\n",
    "    clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)\n",
    "    clahe_image = clahe.apply(gray)\n",
    "\n",
    "    # Convert back to RGB if required\n",
    "    if save_as_rgb:\n",
    "        clahe_image_rgb = cv2.cvtColor(clahe_image, cv2.COLOR_GRAY2RGB)\n",
    "    else:\n",
    "        clahe_image_rgb = clahe_image\n",
    "\n",
    "    # Save the image if a save_path is provided\n",
    "    if save_path:\n",
    "        cv2.imwrite(save_path, clahe_image_rgb if save_as_rgb else clahe_image)\n",
    "        print(f\"CLAHE-applied image saved at {save_path}\")\n",
    "\n",
    "    return clahe_image_rgb if save_as_rgb else clahe_image\n"
   ],
   "id": "cb8da00b65162670",
   "outputs": [],
   "execution_count": 108
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:20.985851Z",
     "start_time": "2025-01-06T11:16:20.970205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def reduce_brightness(image_path, save_path, reduction_factor=0.5):\n",
    "    \"\"\"\n",
    "    Reads an image, reduces its brightness, and saves the result.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_path: Path to the input image.\n",
    "    - save_path: Path to save the output image.\n",
    "    - reduction_factor: Factor by which to reduce brightness (0.0 to 1.0, where 1.0 is no change).\n",
    "    \n",
    "    Returns:\n",
    "    - The brightness-reduced image as a NumPy array.\n",
    "    \"\"\"\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Image at path {image_path} could not be loaded.\")\n",
    "\n",
    "    # Convert to float to avoid clipping during multiplication\n",
    "    image = image.astype(np.float32)\n",
    "\n",
    "    # Reduce brightness\n",
    "    brightness_reduced = np.clip(image * reduction_factor, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # Save the result\n",
    "    cv2.imwrite(save_path, brightness_reduced)\n",
    "    print(f\"Brightness-reduced image saved at {save_path}\")\n",
    "\n",
    "    return brightness_reduced\n"
   ],
   "id": "481263388286cb23",
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:21.017220Z",
     "start_time": "2025-01-06T11:16:21.001488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_background(image_path, save_path):\n",
    "    # Processing the image \n",
    "    input = Image.open(image_path)\n",
    "    \n",
    "    # Removing the background from the given Image \n",
    "    output = remove(input)\n",
    "    \n",
    "    # Saving the image in the given path\n",
    "    output.save(save_path)"
   ],
   "id": "f7ff38527823b67f",
   "outputs": [],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:21.048656Z",
     "start_time": "2025-01-06T11:16:21.033034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gaussian_smoothing(image, kernel_size=(5, 5), sigma=0):\n",
    "    \"\"\"\n",
    "    Applies Gaussian smoothing to an image.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: Input image (grayscale or BGR/RGB).\n",
    "    - kernel_size: Size of the Gaussian kernel (height, width).\n",
    "    - sigma: Standard deviation of the Gaussian distribution.\n",
    "    \n",
    "    Returns:\n",
    "    - Smoothed image.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(f\"../../resources/stylized-pictures/no_background/{image_name}_background_removed.png\")\n",
    "    # Apply median blur\n",
    "    median_blur = cv2.medianBlur(image, 5)\n",
    "    #smoothed = cv2.GaussianBlur(image, kernel_size, sigma)\n",
    "    # Save the smoothed image\n",
    "    cv2.imwrite(f\"../../resources/stylized-pictures/no_background/smoothed/{image_name}_smoothed.png\", median_blur)\n",
    "    return f\"../../resources/stylized-pictures/no_background/smoothed/{image_name}_smoothed.png\"\n",
    "    "
   ],
   "id": "28d5f43487d7f754",
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:21.085405Z",
     "start_time": "2025-01-06T11:16:21.064687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def remove_temperature_boxes(image_path, image_name):\n",
    "    # Load the image using OpenCV\n",
    "    image_cv = cv2.imread(image_path)\n",
    "\n",
    "    # Convert to HSV to target specific colors (red and green boxes)\n",
    "    hsv_image = cv2.cvtColor(image_cv, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Define color ranges for red and green (in HSV space)\n",
    "    lower_red1 = np.array([0, 120, 70])\n",
    "    upper_red1 = np.array([10, 255, 255])\n",
    "    lower_red2 = np.array([170, 120, 70])\n",
    "    upper_red2 = np.array([180, 255, 255])\n",
    "    lower_green = np.array([35, 100, 100])\n",
    "    upper_green = np.array([85, 255, 255])\n",
    "\n",
    "    # Create masks for red and green\n",
    "    mask_red1 = cv2.inRange(hsv_image, lower_red1, upper_red1)\n",
    "    mask_red2 = cv2.inRange(hsv_image, lower_red2, upper_red2)\n",
    "    mask_green = cv2.inRange(hsv_image, lower_green, upper_green)\n",
    "    mask_red = mask_red1 | mask_red2\n",
    "\n",
    "    # Combine masks for both colors\n",
    "    mask_combined = mask_red | mask_green\n",
    "\n",
    "    # Dilate the mask slightly to cover edges of the red and green regions\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    dilated_mask = cv2.dilate(mask_combined, kernel, iterations=1)\n",
    "\n",
    "    # Inpaint to remove red and green regions\n",
    "    image_no_boxes = cv2.inpaint(image_cv, dilated_mask, inpaintRadius=5, flags=cv2.INPAINT_TELEA)\n",
    "\n",
    "    # Apply Gaussian blur to smooth the entire image\n",
    "    smoothed_image = cv2.GaussianBlur(image_no_boxes, (5, 5), 0)\n",
    "\n",
    "    # Save the final image\n",
    "    smoothed_output_path = f\"../../resources/stylized-pictures/no_boxes/{image_name}_no_boxes.png\"\n",
    "    cv2.imwrite(smoothed_output_path, smoothed_image)\n",
    "\n",
    "    return smoothed_output_path\n",
    "\n"
   ],
   "id": "e971e56c123f0501",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:21.117251Z",
     "start_time": "2025-01-06T11:16:21.101984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def mask_temperature_scale(image_path, output_path):\n",
    "    \"\"\"\n",
    "    Masks the temperature scale on the left side of the image\n",
    "    by replacing it with the average background color.\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Define the regions to mask\n",
    "    scale_width = 210  # Width of the scale region on the left\n",
    "    temp_box_width = 260  # Width of the temperature box in the top-right\n",
    "    temp_box_height = 90  # Height of the temperature box in the top-right\n",
    "\n",
    "    # Mask the left temperature scale\n",
    "    scale_region = image[:, :scale_width]\n",
    "    avg_color_scale = np.mean(image[:, scale_width:], axis=(0, 1), dtype=int)\n",
    "    image[:, :scale_width] = avg_color_scale\n",
    "\n",
    "    # Mask the temperature box in the upper-right corner\n",
    "    temp_box_region = image[:temp_box_height, -temp_box_width:]\n",
    "    avg_color_temp_box = np.mean(image[temp_box_height:, :-temp_box_width], axis=(0, 1), dtype=int)\n",
    "    image[:temp_box_height, -temp_box_width:] = avg_color_temp_box\n",
    "\n",
    "     # Save the masked image\n",
    "    cv2.imwrite(output_path, image)\n",
    "\n",
    "    return output_path"
   ],
   "id": "9074bbc0da338447",
   "outputs": [],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:21.149524Z",
     "start_time": "2025-01-06T11:16:21.133221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def detect_edges_sobelX(image):\n",
    "    \"\"\"Detects edges in an image using the Sobel edge detection algorithm\"\"\"\n",
    "    return cv2.Sobel(image,cv2.CV_64F,1,0,ksize=5)\n",
    "\n",
    "def detect_edges_sobelY(image):\n",
    "    \"\"\"Detects edges in an image using the Sobel edge detection algorithm\"\"\"\n",
    "    return cv2.Sobel(image,cv2.CV_64F,0,1,ksize=5)"
   ],
   "id": "84866f49999f39ca",
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:21.181475Z",
     "start_time": "2025-01-06T11:16:21.165837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def canny_edge_detection(frame): \n",
    "    # Convert the frame to grayscale for edge detection \n",
    "    # gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) \n",
    "      \n",
    "    # Apply Gaussian blur to reduce noise and smoothen edges \n",
    "    blurred = cv2.GaussianBlur(src=frame, ksize=(3, 5), sigmaX=0.5) \n",
    "      \n",
    "    # Perform Canny edge detection \n",
    "    edges = cv2.Canny(blurred, 70, 135) \n",
    "      \n",
    "    return blurred, edges"
   ],
   "id": "608c4352109ebf66",
   "outputs": [],
   "execution_count": 115
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:21.213314Z",
     "start_time": "2025-01-06T11:16:21.197676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check whether recognized hands are left or right\n",
    "def is_right_hand(landmarks):\n",
    "    counter = 0\n",
    "    \n",
    "    for landmark in landmarks:\n",
    "        if landmark.x > 0.5:\n",
    "            counter += 1\n",
    "            \n",
    "    return counter > 10"
   ],
   "id": "ea710006e172b12e",
   "outputs": [],
   "execution_count": 116
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Data",
   "id": "4a58957232e564bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:21.244706Z",
     "start_time": "2025-01-06T11:16:21.228947Z"
    }
   },
   "cell_type": "code",
   "source": "results = []",
   "id": "b47374467ec521f6",
   "outputs": [],
   "execution_count": 117
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Processing Images",
   "id": "b60bd7a4774aa302"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:16:21.276228Z",
     "start_time": "2025-01-06T11:16:21.260237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from rembg import remove \n",
    "from PIL import Image"
   ],
   "id": "3391c11d5b67060b",
   "outputs": [],
   "execution_count": 118
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T12:07:29.004154Z",
     "start_time": "2025-01-06T12:07:28.972880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_cross(image_path, image_name):\n",
    "    \"\"\"\n",
    "    Removes the white cross from the center of an image by creating a mask and applying inpainting.\n",
    "    \"\"\"\n",
    "    # Load the image in color\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Get image dimensions\n",
    "    h, w = image.shape[:2]\n",
    "    center_x, center_y = w // 2, h // 2\n",
    "    \n",
    "    # Define cross size (adjust as needed)\n",
    "    cross_size = 50  # Length of the cross arms\n",
    "    thickness = 10   # Thickness of the lines\n",
    "\n",
    "    # Create a mask for the cross\n",
    "    mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    # Draw horizontal and vertical lines for the cross\n",
    "    cv2.line(mask, (center_x - cross_size, center_y), (center_x + cross_size, center_y), 255, thickness)\n",
    "    cv2.line(mask, (center_x, center_y - cross_size), (center_x, center_y + cross_size), 255, thickness)\n",
    "\n",
    "    # Apply inpainting to remove the cross\n",
    "    result = cv2.inpaint(image, mask, inpaintRadius=5, flags=cv2.INPAINT_TELEA)\n",
    "    \n",
    "    path = f\"../../resources/stylized-pictures/no_cross/{image_name}_no_cross.png\"\n",
    "    cv2.imwrite(path, result)\n",
    "    \n",
    "    return path\n"
   ],
   "id": "67d188352ad94c68",
   "outputs": [],
   "execution_count": 168
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T12:07:29.463191Z",
     "start_time": "2025-01-06T12:07:29.447495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def execute_transformation_pipeline(image_path, image_name, colorization_model='siggraph17'):\n",
    "    no_boxes_image_path = remove_temperature_boxes(image_path, image_name)\n",
    "    \n",
    "    # # Invert the colors\n",
    "    inverted_image = cv2.bitwise_not(cv2.imread(no_boxes_image_path))\n",
    "    # Save the inverted image\n",
    "    inverted_image_path = f\"../../resources/stylized-pictures/inverted/{image_name}_inverted.png\"\n",
    "    cv2.imwrite(inverted_image_path, inverted_image)\n",
    "\n",
    "    \n",
    "    # Generate colorized image using Zhang model\n",
    "    transformed_image_path = None\n",
    "\n",
    "    if colorization_model == 'eccv16':\n",
    "        transformed_image_path = colorize_zhang_eccv16(inverted_image_path, image_name)\n",
    "    elif colorization_model == 'siggraph17':\n",
    "        transformed_image_path = colorize_zhang_siggraph17(inverted_image_path, image_name)\n",
    "    \n",
    "    return transformed_image_path\n",
    "    "
   ],
   "id": "c1db65c787b519f2",
   "outputs": [],
   "execution_count": 169
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T12:07:30.031044Z",
     "start_time": "2025-01-06T12:07:30.015423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def execute_transformation_pipeline_with_no_visible_fingers(image_path, image_name, colorization_model='siggraph17'):\n",
    "    no_boxes_image_path = remove_temperature_boxes(image_path, image_name)\n",
    "    # no_cross_path = remove_cross(no_boxes_image_path, image_name)\n",
    "    \n",
    "    image = cv2.imread(no_boxes_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    \n",
    "    # Step 3: Enhance contrast using CLAHE\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    enhanced_image = clahe.apply(image)\n",
    "    \n",
    "    # Step 4: Sharpen edges to highlight fingers\n",
    "    kernel = np.array([[-1, -1, -1],\n",
    "                       [-1,  9, -1],\n",
    "                       [-1, -1, -1]])\n",
    "    sharpened_image = cv2.filter2D(enhanced_image, -1, kernel)\n",
    "    \n",
    "    # Change the image to RGB format\n",
    "    sharpened_image = cv2.cvtColor(sharpened_image, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    sharpened_image_path = f\"../../resources/stylized-pictures/not_detected/sharpened/{image_name}_sharpened.png\"\n",
    "    cv2.imwrite(sharpened_image_path, sharpened_image)\n",
    "    \n",
    "    # transformed_image_path = colorize_zhang_siggraph17(sharpened_image_path, image_name)\n",
    "    \n",
    "    return sharpened_image_path\n",
    "    "
   ],
   "id": "77c8625a2aac3113",
   "outputs": [],
   "execution_count": 170
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T12:07:30.699004Z",
     "start_time": "2025-01-06T12:07:30.672273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def two_hands_detected(image_path):\n",
    "    image = mp.Image.create_from_file(image_path)\n",
    "    \n",
    "    results = gesture_recognizer.recognize(image)\n",
    "    \n",
    "    if not results.hand_landmarks or len(results.hand_landmarks) < 2:\n",
    "        return False\n",
    "    return True"
   ],
   "id": "a01c03a8ea31e254",
   "outputs": [],
   "execution_count": 171
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T12:07:31.058266Z",
     "start_time": "2025-01-06T12:07:31.042628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_landmarks_from_results(results):\n",
    "    landmarks = results.hand_landmarks\n",
    "    gestures = results.gestures\n",
    "    final_landmarks = {}\n",
    "    \n",
    "    for i, hand_landmarks in enumerate(landmarks):\n",
    "        hand_gestures = gestures[i]\n",
    "        handedness = 'Right' if is_right_hand(hand_landmarks) else 'Left'\n",
    "        \n",
    "        final_landmarks[handedness] = {\"landmarks\": hand_landmarks, \"gestures\": hand_gestures[0].category_name, \"score\": hand_gestures[0].score}\n",
    "\n",
    "    \n",
    "    return final_landmarks\n",
    "    "
   ],
   "id": "9ac257bc11291c7a",
   "outputs": [],
   "execution_count": 172
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T12:07:31.755755Z",
     "start_time": "2025-01-06T12:07:31.740113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def annotate_landmarks(image_path, results, output_path):\n",
    "    try:\n",
    "        image = cv2.imread(image_path)\n",
    "        annotated_image = np.copy(image)\n",
    "        \n",
    "        for hand in results:\n",
    "            hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "            hand_landmarks_proto.landmark.extend([\n",
    "                landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand\n",
    "            ])\n",
    "            solutions.drawing_utils.draw_landmarks(\n",
    "                annotated_image,\n",
    "                hand_landmarks_proto,\n",
    "                solutions.hands.HAND_CONNECTIONS,\n",
    "                solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "                solutions.drawing_styles.get_default_hand_connections_style()\n",
    "            )\n",
    "        \n",
    "        cv2.imwrite(output_path, annotated_image)\n",
    "    except Exception as e:\n",
    "        print(f\"COULD NOT ANNOTATE LANDMARKS FOR {image_path.upper()}: {str(e).upper()}\")\n"
   ],
   "id": "6152a51b1ad97a81",
   "outputs": [],
   "execution_count": 173
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T12:07:32.370533Z",
     "start_time": "2025-01-06T12:07:32.355274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def rotate_landmarks_90_counterclockwise(landmarks):\n",
    "    rotated_landmarks = []\n",
    "    for group in landmarks:\n",
    "        rotated_group = []\n",
    "        for point in group:\n",
    "            x, y = point['x'], point['y']\n",
    "            rotated_group.append({'x': y, 'y': 1 - x})\n",
    "        rotated_landmarks.append(rotated_group)\n",
    "    return rotated_landmarks"
   ],
   "id": "a37db6940fa5dd2c",
   "outputs": [],
   "execution_count": 174
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T13:29:28.019056Z",
     "start_time": "2025-01-06T13:29:28.003449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_transformed_siggraph17 = {\"Left\": [], \"Right\": []}\n",
    "total_transformed_eccv16 = {\"Left\": [], \"Right\": []}"
   ],
   "id": "654ccec865b177e5",
   "outputs": [],
   "execution_count": 224
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T13:29:28.176320Z",
     "start_time": "2025-01-06T13:29:28.160578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_distances_siggaph17 = []\n",
    "total_distances_eccv16 = []"
   ],
   "id": "c37319ec0fa2f07",
   "outputs": [],
   "execution_count": 225
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T13:29:28.320293Z",
     "start_time": "2025-01-06T13:29:28.302336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hand_landmarker_first = {\"Left\": [], \"Right\": []}\n",
    "hand_landmarker_second = {\"Left\": [], \"Right\": []}\n",
    "\n",
    "gesture_recognizer_first = {\"Left\": [], \"Right\": []}\n",
    "gesture_recognizer_second = {\"Left\": [], \"Right\": []}"
   ],
   "id": "e45e013960952490",
   "outputs": [],
   "execution_count": 226
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T13:29:28.453812Z",
     "start_time": "2025-01-06T13:29:28.433348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def advanced_enhancement(image_path, image_name):\n",
    "    no_boxes_image_path = remove_temperature_boxes(image_path, image_name)\n",
    "    no_cross_path = remove_cross(no_boxes_image_path, image_name)\n",
    "    # Load the image in grayscale\n",
    "    image = cv2.imread(no_cross_path, cv2.IMREAD_GRAYSCALE)\n",
    "    # Apply histogram equalization\n",
    "    equalized = cv2.equalizeHist(image)\n",
    "    # Invert the colors\n",
    "    inverted = cv2.bitwise_not(equalized)\n",
    "    # Apply GaussianBlur to reduce noise and enhance features\n",
    "    blurred = cv2.GaussianBlur(inverted, (5, 5), 0)\n",
    "    # Combine equalization and inversion for better contrast\n",
    "    combined = cv2.addWeighted(equalized, 0.7, blurred, 0.3, 0)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    enhanced_image = clahe.apply(combined)\n",
    "    \n",
    "    # Change the image to RGB format\n",
    "    # sharpened_image = cv2.cvtColor(sharpened_image, cv2.COLOR_GRAY2RGB)\n",
    "    # Save the enhanced image\n",
    "    enhanced_path = f\"../../resources/stylized-pictures/enhanced/{image_name}_enhanced.png\"\n",
    "    cv2.imwrite(enhanced_path, cv2.cvtColor(enhanced_image, cv2.COLOR_GRAY2RGB))\n",
    "\n",
    "    return enhanced_path"
   ],
   "id": "a78646c5bf10bb49",
   "outputs": [],
   "execution_count": 227
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T13:29:28.582596Z",
     "start_time": "2025-01-06T13:29:28.551349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Category:\n",
    "    def __init__(self, category_name):\n",
    "        self.category_name = category_name"
   ],
   "id": "3f52461dd1dadf7a",
   "outputs": [],
   "execution_count": 228
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T13:29:28.724327Z",
     "start_time": "2025-01-06T13:29:28.692910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Results:\n",
    "    def __init__(self, hand_landmarks, handedness):\n",
    "        self.hand_landmarks = hand_landmarks\n",
    "        self.handedness = handedness"
   ],
   "id": "2344969eee6061f7",
   "outputs": [],
   "execution_count": 229
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T13:29:28.875451Z",
     "start_time": "2025-01-06T13:29:28.844050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Process each image\n",
    "# for file_name in os.listdir(IR_ground_truth_directory):\n",
    "#     if file_name.endswith('.json'):\n",
    "#         json_file_path = os.path.join(IR_ground_truth_directory, file_name)\n",
    "#         \n",
    "#         with open(json_file_path, 'r') as f:\n",
    "#             ground_truth_data = json.load(f)\n",
    "#             \n",
    "#         for entry in ground_truth_data:\n",
    "#             image_name = entry[\"image\"]\n",
    "#             ground_truth_landmarks = entry[\"landmarks\"]\n",
    "#             ground_truth_landmarks = rotate_landmarks_90_counterclockwise(ground_truth_landmarks)\n",
    "#             image_path = f\"{IR_image_directory}/{image_name}\"\n",
    "#             \n",
    "#             image = cv2.imread(image_path)\n",
    "#             rotated = cv2.rotate(image, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "#             rotated_path = f\"../../resources/stylized-pictures/rotated/{image_name}_rotated.png\"\n",
    "#             cv2.imwrite(rotated_path, rotated)\n",
    "#             image_path = rotated_path\n",
    "#             \n",
    "#             \n",
    "#             print(f\"Processing image: {image_name}\")\n",
    "# \n",
    "#             first_pipeline_image_path = execute_transformation_pipeline(image_path, image_name)\n",
    "#             first_pipeline_score = assess_accuracy(first_pipeline_image_path, ground_truth_landmarks)\n",
    "#             print(f\"Results for original pipeline - Left: {np.mean(first_pipeline_score['Left']):.2f}%, Right: {np.mean(first_pipeline_score['Right']):.2f}%\")\n",
    "#             \n",
    "#             second_pipeline_image_path = execute_transformation_pipeline_with_no_visible_fingers(image_path, image_name)\n",
    "#             second_pipeline_score = assess_accuracy(second_pipeline_image_path, ground_truth_landmarks)\n",
    "#             print(f\"Results for second pipeline - Left: {np.mean(second_pipeline_score['Left']):.2f}%, Right: {np.mean(second_pipeline_score['Right']):.2f}%\")\n",
    "#             \n",
    "#             third_pipeline_image_path = advanced_enhancement(image_path, image_name)\n",
    "#             third_pipeline_score = assess_accuracy(third_pipeline_image_path, ground_truth_landmarks)\n",
    "#             print(f\"Results for third pipeline - Left: {np.mean(third_pipeline_score['Left']):.2f}%, Right: {np.mean(third_pipeline_score['Right']):.2f}%\")\n",
    "#             \n",
    "#             image = mp.Image.create_from_file(first_pipeline_image_path)\n",
    "#             recognizer_results_first_pipeline = gesture_recognizer.recognize(image)\n",
    "#             landmarks_first_pipeline = create_landmarks_from_results(recognizer_results_first_pipeline)\n",
    "#             \n",
    "#             image = mp.Image.create_from_file(second_pipeline_image_path)\n",
    "#             recognizer_results_second_pipeline = gesture_recognizer.recognize(image)\n",
    "#             landmarks_second_pipeline = create_landmarks_from_results(recognizer_results_second_pipeline)\n",
    "#             \n",
    "#             final_score = first_pipeline_score\n",
    "#             final_landmarks = []\n",
    "#             \n",
    "#             if len(recognizer_results_first_pipeline.gestures) == 2 and len(recognizer_results_second_pipeline.gestures) == 2:\n",
    "#                 mapped_indexes = {\"Left\": 0, \"Right\": 1}\n",
    "#                 for hand in [\"Left\", \"Right\"]:\n",
    "#                     index = mapped_indexes[hand]\n",
    "#                     first_pipeline_gesture = recognizer_results_first_pipeline.gestures[index][0]\n",
    "#                     second_pipeline_gesture = recognizer_results_second_pipeline.gestures[index][0]\n",
    "#                     \n",
    "#                     if first_pipeline_gesture.category_name == 'Open_Palm' and second_pipeline_gesture.category_name == 'Open_Palm':\n",
    "#                         # Merge the landmarks\n",
    "#                         print(\"Both recognized open palm\")\n",
    "#                         merged_left_landmarks = merge_landmarks(landmarks_first_pipeline[\"Left\"], landmarks_second_pipeline[\"Left\"], \"Left\")\n",
    "#                         merged_right_landmarks = merge_landmarks(landmarks_first_pipeline[\"Right\"], landmarks_second_pipeline[\"Right\"], \"Right\")\n",
    "#                         final_landmarks.append(merged_left_landmarks)\n",
    "#                         final_landmarks.append(merged_right_landmarks)\n",
    "#                         # if first_pipeline_gesture.score > second_pipeline_gesture.score:\n",
    "#                         #     final_score[hand] = first_pipeline_score[hand]\n",
    "#                         #     final_landmarks.append(landmarks_first_pipeline[hand])\n",
    "#                         # else:\n",
    "#                         #     final_score[hand] = second_pipeline_score[hand]\n",
    "#                         #     final_landmarks.append(landmarks_second_pipeline[hand])\n",
    "#                         \n",
    "#                     elif first_pipeline_gesture.category_name == 'Open_Palm':\n",
    "#                         print(\"First recognized open palm\")\n",
    "#                         final_score[hand] = first_pipeline_score[hand]\n",
    "#                         final_landmarks.append(landmarks_first_pipeline[hand])\n",
    "#                     elif second_pipeline_gesture.category_name == 'Open_Palm':\n",
    "#                         print(\"Second recognized open palm\")\n",
    "#                         final_score[hand] = second_pipeline_score[hand]\n",
    "#                         final_landmarks.append(landmarks_second_pipeline[hand])\n",
    "#                     else:\n",
    "#                         print(\"Neither recognized open palm\")\n",
    "#                         if first_pipeline_gesture.score < second_pipeline_gesture.score:\n",
    "#                             final_score[hand] = first_pipeline_score[hand]\n",
    "#                             final_landmarks.append(landmarks_first_pipeline[hand])\n",
    "#                         else:\n",
    "#                             final_score = second_pipeline_score\n",
    "#                             final_landmarks.append(landmarks_second_pipeline[hand])\n",
    "#                         \n",
    "#             if len(recognizer_results_first_pipeline.gestures) == 2 and len(recognizer_results_second_pipeline.gestures) == 1:\n",
    "#                 second_pipeline_handedness = \"Right\" if is_right_hand(recognizer_results_second_pipeline.hand_landmarks[0]) else \"Left\"\n",
    "#                 complementary_hand = \"Right\" if second_pipeline_handedness == \"Left\" else \"Left\"\n",
    "#                 index = 0 if second_pipeline_handedness == \"Left\" else 1\n",
    "#                 first_pipeline_gesture = recognizer_results_first_pipeline.gestures[index][0]\n",
    "#                 second_pipeline_gesture = recognizer_results_second_pipeline.gestures[0][0]\n",
    "#                 \n",
    "#                 if first_pipeline_gesture == 'Open_Palm' and second_pipeline_gesture == 'Open_Palm' and first_pipeline_gesture.score > second_pipeline_gesture.score:\n",
    "#                     final_score = first_pipeline_score\n",
    "#                     final_landmarks.append(landmarks_first_pipeline[\"Left\"])\n",
    "#                     final_landmarks.append(landmarks_first_pipeline[\"Right\"])\n",
    "#                 elif first_pipeline_gesture == 'Open_Palm' and second_pipeline_gesture == 'Open_Palm' and first_pipeline_gesture.score < second_pipeline_gesture.score:\n",
    "#                     final_score[second_pipeline_handedness] = second_pipeline_score[second_pipeline_handedness]\n",
    "#                     final_landmarks.append(landmarks_second_pipeline[second_pipeline_handedness])\n",
    "#                     final_landmarks.append(landmarks_first_pipeline[complementary_hand])\n",
    "#                 elif first_pipeline_gesture == 'Open_Palm':\n",
    "#                     final_score = first_pipeline_score\n",
    "#                     final_landmarks.append(landmarks_first_pipeline[\"Left\"])\n",
    "#                     final_landmarks.append(landmarks_first_pipeline[\"Right\"])\n",
    "#                 elif second_pipeline_gesture == 'Open_Palm':\n",
    "#                     final_score[second_pipeline_handedness] = second_pipeline_score[second_pipeline_handedness]\n",
    "#                     final_landmarks.append(landmarks_second_pipeline[second_pipeline_handedness])\n",
    "#                     final_landmarks.append(landmarks_first_pipeline[complementary_hand])\n",
    "#                 else: # None of them are open palm, take the None gesture with less score\n",
    "#                     if first_pipeline_gesture.score < second_pipeline_gesture.score:\n",
    "#                         final_score = first_pipeline_score\n",
    "#                         final_landmarks.append(landmarks_first_pipeline[\"Left\"])\n",
    "#                         final_landmarks.append(landmarks_first_pipeline[\"Right\"])\n",
    "#                     else:\n",
    "#                         final_score[second_pipeline_handedness] = second_pipeline_score[second_pipeline_handedness] \n",
    "#                         final_landmarks.append(landmarks_second_pipeline[second_pipeline_handedness])\n",
    "#                         final_landmarks.append(landmarks_first_pipeline[complementary_hand])\n",
    "#                 \n",
    "#             if len(recognizer_results_first_pipeline.gestures) == 1 and len(recognizer_results_second_pipeline.gestures) == 2:\n",
    "#                 first_pipeline_handedness = \"Right\" if is_right_hand(recognizer_results_first_pipeline.hand_landmarks[0]) else \"Left\"\n",
    "#                 complementary_hand = \"Right\" if first_pipeline_handedness == \"Left\" else \"Left\"\n",
    "#                 index = 0 if first_pipeline_handedness == \"Left\" else 1\n",
    "#                 first_pipeline_gesture = recognizer_results_first_pipeline.gestures[0][0]\n",
    "#                 second_pipeline_gesture = recognizer_results_second_pipeline.gestures[index][0]\n",
    "#                 \n",
    "#                 final_score = second_pipeline_score \n",
    "#                 \n",
    "#                 if first_pipeline_gesture.category_name == 'Open_Palm' and second_pipeline_gesture.category_name == 'Open_Palm' and first_pipeline_gesture.score > second_pipeline_gesture.score:\n",
    "#                     final_score[first_pipeline_handedness] = first_pipeline_score[first_pipeline_handedness]\n",
    "#                     final_score[complementary_hand] = second_pipeline_score[complementary_hand]\n",
    "#                     final_landmarks.append(landmarks_first_pipeline[first_pipeline_handedness])\n",
    "#                     final_landmarks.append(landmarks_second_pipeline[complementary_hand])\n",
    "#                     \n",
    "#                 elif first_pipeline_gesture.category_name == 'Open_Palm' and second_pipeline_gesture.category_name == 'Open_Palm' and first_pipeline_gesture.score < second_pipeline_gesture.score:\n",
    "#                     final_score[first_pipeline_handedness] = second_pipeline_score[first_pipeline_handedness]\n",
    "#                     final_score[complementary_hand] = second_pipeline_score[complementary_hand]\n",
    "#                     final_landmarks.append(landmarks_second_pipeline[first_pipeline_handedness])\n",
    "#                     final_landmarks.append(landmarks_second_pipeline[complementary_hand])\n",
    "#                 elif first_pipeline_gesture.category_name == 'Open_Palm':\n",
    "#                     final_score[first_pipeline_handedness] = first_pipeline_score[first_pipeline_handedness]\n",
    "#                     final_score[complementary_hand] = second_pipeline_score[complementary_hand]\n",
    "#                     final_landmarks.append(landmarks_first_pipeline[first_pipeline_handedness])\n",
    "#                     final_landmarks.append(landmarks_second_pipeline[complementary_hand])\n",
    "#                 elif second_pipeline_gesture.category_name == 'Open_Palm':\n",
    "#                     final_score = second_pipeline_score\n",
    "#                     final_landmarks.append(landmarks_second_pipeline[first_pipeline_handedness])\n",
    "#                     final_landmarks.append(landmarks_second_pipeline[complementary_hand])\n",
    "#                 else:  # None of them are open palm, take the None gesture with less score\n",
    "#                     if first_pipeline_gesture.score < second_pipeline_gesture.score:\n",
    "#                         final_score[first_pipeline_handedness] = first_pipeline_score[first_pipeline_handedness]\n",
    "#                         final_score[complementary_hand] = second_pipeline_score[complementary_hand]\n",
    "#                         final_landmarks.append(landmarks_first_pipeline[first_pipeline_handedness])\n",
    "#                         final_landmarks.append(landmarks_second_pipeline[complementary_hand])\n",
    "#                     else:\n",
    "#                         final_score = second_pipeline_score\n",
    "#                         final_landmarks.append(landmarks_second_pipeline[first_pipeline_handedness])\n",
    "#                         final_landmarks.append(landmarks_second_pipeline[complementary_hand])\n",
    "#                         \n",
    "#             if len(recognizer_results_first_pipeline.gestures) == 1 and len(recognizer_results_second_pipeline.gestures) == 1:\n",
    "#                 first_pipeline_handedness = \"Right\" if is_right_hand(recognizer_results_first_pipeline.hand_landmarks[0]) else \"Left\"\n",
    "#                 second_pipeline_handedness = \"Right\" if is_right_hand(recognizer_results_second_pipeline.hand_landmarks[0]) else \"Left\"\n",
    "#                 \n",
    "#                 if first_pipeline_handedness != second_pipeline_handedness:\n",
    "#                     final_score[first_pipeline_handedness] = first_pipeline_score[first_pipeline_handedness]\n",
    "#                     final_score[second_pipeline_handedness] = second_pipeline_score[second_pipeline_handedness]\n",
    "#                     final_landmarks.append(landmarks_first_pipeline[first_pipeline_handedness])\n",
    "#                     final_landmarks.append(landmarks_second_pipeline[second_pipeline_handedness])\n",
    "#                 else:\n",
    "#                     first_pipeline_gesture = recognizer_results_first_pipeline.gestures[0][0]\n",
    "#                     second_pipeline_gesture = recognizer_results_second_pipeline.gestures[0][0]\n",
    "#                     \n",
    "#                     if first_pipeline_gesture.category_name == 'Open_Palm' and second_pipeline_gesture.category_name == 'Open_Palm' and first_pipeline_gesture.score > second_pipeline_gesture.score:\n",
    "#                         final_score = first_pipeline_score\n",
    "#                         final_landmarks.append(landmarks_first_pipeline[first_pipeline_handedness])\n",
    "#                     elif first_pipeline_gesture.category_name == 'Open_Palm' and second_pipeline_gesture.category_name == 'Open_Palm' and first_pipeline_gesture.score < second_pipeline_gesture.score:\n",
    "#                         final_score = second_pipeline_score\n",
    "#                         final_landmarks.append(landmarks_second_pipeline[second_pipeline_handedness])\n",
    "#                     elif first_pipeline_gesture.category_name == 'Open_Palm':\n",
    "#                         final_score = first_pipeline_score\n",
    "#                         final_landmarks.append(landmarks_first_pipeline[first_pipeline_handedness])\n",
    "#                     elif second_pipeline_gesture.category_name == 'Open_Palm':\n",
    "#                         final_score = second_pipeline_score\n",
    "#                         final_landmarks.append(landmarks_second_pipeline[second_pipeline_handedness])\n",
    "#                     else:\n",
    "#                         if first_pipeline_gesture.score < second_pipeline_gesture.score:\n",
    "#                             final_score = first_pipeline_score\n",
    "#                             final_landmarks.append(landmarks_first_pipeline[first_pipeline_handedness])\n",
    "#                         else:\n",
    "#                             final_score = second_pipeline_score\n",
    "#                             final_landmarks.append(landmarks_second_pipeline[second_pipeline_handedness])\n",
    "#                             \n",
    "#             if len(recognizer_results_first_pipeline.gestures) == 0 :\n",
    "#                 final_score = second_pipeline_score\n",
    "#                 final_landmarks.append(landmarks_second_pipeline[\"Left\"])\n",
    "#                 final_landmarks.append(landmarks_second_pipeline[\"Right\"])\n",
    "#                 \n",
    "#             if len(recognizer_results_second_pipeline.gestures) == 0:\n",
    "#                 final_score = first_pipeline_score\n",
    "#                 final_landmarks.append(landmarks_first_pipeline[\"Left\"])\n",
    "#                 final_landmarks.append(landmarks_first_pipeline[\"Right\"])\n",
    "#                         \n",
    "#                         \n",
    "#             final_landmark_accuracy = calculate_hand_accuracies(ground_truth_landmarks, final_landmarks, [\"Left\", \"Right\"], distance_threshold)\n",
    "#             final_landmark_accuracy[\"Left\"] = [0.0]*21 if np.isnan(np.mean(final_landmark_accuracy[\"Left\"])) else final_landmark_accuracy[\"Left\"]\n",
    "#             final_landmark_accuracy[\"Right\"] = [0.0]*21 if np.isnan(np.mean(final_landmark_accuracy[\"Right\"])) else final_landmark_accuracy[\"Right\"]\n",
    "#             print(f\"Results for final score - Left: {np.mean(final_landmark_accuracy['Left']):.2f}%, Right: {np.mean(final_landmark_accuracy['Right']):.2f}%\")\n",
    "#             \n",
    "#             # Check if the final landmark accuracies and the final score are the same\n",
    "#             # if final_landmark_accuracy[\"Left\"] != final_score[\"Left\"] or final_landmark_accuracy[\"Right\"] != final_score[\"Right\"]:\n",
    "#             #     print(\"Final landmark accuracies and final score are not the same!!!!!!\")\n",
    "#             #     print(f\"Final landmark accuracies - Left: {np.mean(final_landmark_accuracy['Left']):.2f}%, Right: {np.mean(final_landmark_accuracy['Right']):.2f}%\")\n",
    "#             #     print(f\"Final score - Left: {np.mean(final_score['Left']):.2f}%, Right: {np.mean(final_score['Right']):.2f}%\")\n",
    "#             \n",
    "#             # Annotate with final landmarks\n",
    "#             annotated_image_path = f\"../../resources/stylized-pictures/annotated/{image_name}_annotated.png\"\n",
    "#             annotate_landmarks(image_path, final_landmarks, annotated_image_path)\n",
    "# \n",
    "#                         \n",
    "#             # print(f\"Results for final score - Left: {np.mean(final_score['Left']):.2f}%, Right: {np.mean(final_score['Right']):.2f}%\")   \n",
    "#             # print(f\"Length of final landmarks - {len(final_landmarks)}\")\n",
    "#             print(\"---------------------------------------------------------\")\n",
    "# \n",
    "#           \n",
    "#             for hand in [\"Left\", \"Right\"]:\n",
    "#                 total_transformed_siggraph17[hand].extend(final_landmark_accuracy[hand])\n",
    "# \n"
   ],
   "id": "3071ab27f6f87b9c",
   "outputs": [],
   "execution_count": 230
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T13:34:02.871965Z",
     "start_time": "2025-01-06T13:34:02.856337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def merge_landmarks(pipelines, handedness):\n",
    "    def is_fingers_correct_order(landmarks):\n",
    "        # Extract x-coordinates for all landmarks\n",
    "        x_coords = [landmarks[i].x for i in range(len(landmarks))]\n",
    "        \n",
    "        if handedness == \"Right\":\n",
    "            # Right hand: all pinky points (6 to 20) should be to the left of the thumb (2, 3, 4)\n",
    "            return all(x_coords[pinky] < x_coords[2] for pinky in range(10, 21))\n",
    "        else:  # Left hand\n",
    "            # Left hand: all pinky points (6 to 20) should be to the right of the thumb (2, 3, 4)\n",
    "            return all(x_coords[pinky] > x_coords[2] for pinky in range(10, 21))\n",
    "\n",
    "    def merge_logic(points, index):\n",
    "        if index < 6:  # For the first 6 landmarks\n",
    "            if handedness == \"Left\":\n",
    "                return min(points, key=lambda p: p.x)  # Leftmost for left hand\n",
    "            else:\n",
    "                return max(points, key=lambda p: p.x)  # Rightmost for right hand\n",
    "        else:  # For the remaining landmarks\n",
    "            return min(points, key=lambda p: p.y)  # Lowest y\n",
    "\n",
    "    correct_pipelines = [pipeline for pipeline in pipelines if is_fingers_correct_order(pipeline['landmarks'])]\n",
    "\n",
    "    # If at least one correct pipeline is found\n",
    "    if correct_pipelines:\n",
    "        merged_landmarks = []\n",
    "        num_landmarks = len(correct_pipelines[0]['landmarks'])\n",
    "\n",
    "        for i in range(num_landmarks):\n",
    "            # Extract corresponding landmarks from correct pipelines\n",
    "            points = [pipeline['landmarks'][i] for pipeline in correct_pipelines]\n",
    "            # Apply merging logic\n",
    "            merged_landmarks.append(merge_logic(points, i))\n",
    "\n",
    "        return merged_landmarks\n",
    "\n",
    "    # No correct pipeline found, fall back to merging all pipelines\n",
    "    merged_landmarks = []\n",
    "    num_landmarks = len(pipelines[0]['landmarks'])\n",
    "\n",
    "    for i in range(num_landmarks):\n",
    "        points = [pipeline['landmarks'][i] for pipeline in pipelines]\n",
    "        merged_landmarks.append(merge_logic(points, i))\n",
    "\n",
    "    return merged_landmarks\n"
   ],
   "id": "614ac04d491ee3d",
   "outputs": [],
   "execution_count": 235
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T13:33:13.718713Z",
     "start_time": "2025-01-06T13:29:29.109995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Process each image\n",
    "for file_name in os.listdir(IR_ground_truth_directory):\n",
    "    if file_name.endswith('.json'):\n",
    "        json_file_path = os.path.join(IR_ground_truth_directory, file_name)\n",
    "        \n",
    "        with open(json_file_path, 'r') as f:\n",
    "            ground_truth_data = json.load(f)\n",
    "            \n",
    "        for entry in ground_truth_data:\n",
    "            image_name = entry[\"image\"]\n",
    "            ground_truth_landmarks = entry[\"landmarks\"]\n",
    "            ground_truth_landmarks = rotate_landmarks_90_counterclockwise(ground_truth_landmarks)\n",
    "            image_path = f\"{IR_image_directory}/{image_name}\"\n",
    "            \n",
    "            image = cv2.imread(image_path)\n",
    "            rotated = cv2.rotate(image, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "            rotated_path = f\"../../resources/stylized-pictures/rotated/{image_name}_rotated.png\"\n",
    "            cv2.imwrite(rotated_path, rotated)\n",
    "            image_path = rotated_path\n",
    "            \n",
    "            \n",
    "            print(f\"Processing image: {image_name}\")\n",
    "\n",
    "            first_pipeline_image_path = execute_transformation_pipeline(image_path, image_name)\n",
    "            first_pipeline_score = assess_accuracy(first_pipeline_image_path, ground_truth_landmarks)\n",
    "            print(f\"Results for original pipeline - Left: {np.mean(first_pipeline_score['Left']):.2f}%, Right: {np.mean(first_pipeline_score['Right']):.2f}%\")\n",
    "            \n",
    "            second_pipeline_image_path = execute_transformation_pipeline_with_no_visible_fingers(image_path, image_name)\n",
    "            second_pipeline_score = assess_accuracy(second_pipeline_image_path, ground_truth_landmarks)\n",
    "            print(f\"Results for second pipeline - Left: {np.mean(second_pipeline_score['Left']):.2f}%, Right: {np.mean(second_pipeline_score['Right']):.2f}%\")\n",
    "            \n",
    "            third_pipeline_image_path = advanced_enhancement(image_path, image_name)\n",
    "            third_pipeline_score = assess_accuracy(third_pipeline_image_path, ground_truth_landmarks)\n",
    "            print(f\"Results for third pipeline - Left: {np.mean(third_pipeline_score['Left']):.2f}%, Right: {np.mean(third_pipeline_score['Right']):.2f}%\")\n",
    "            \n",
    "            image = mp.Image.create_from_file(first_pipeline_image_path)\n",
    "            recognizer_results_first_pipeline = gesture_recognizer.recognize(image)\n",
    "            first_pipeline_gesture_score_landmarks = create_landmarks_from_results(recognizer_results_first_pipeline)\n",
    "            \n",
    "            image = mp.Image.create_from_file(second_pipeline_image_path)\n",
    "            recognizer_results_second_pipeline = gesture_recognizer.recognize(image)\n",
    "            second_pipeline_gesture_score_landmarks = create_landmarks_from_results(recognizer_results_second_pipeline)\n",
    "            \n",
    "            image = mp.Image.create_from_file(third_pipeline_image_path)\n",
    "            recognizer_results_third_pipeline = gesture_recognizer.recognize(image)\n",
    "            third_pipeline_gesture_score_landmarks = create_landmarks_from_results(recognizer_results_third_pipeline)\n",
    "            \n",
    "            # Initialize landmarks_gestures_score dictionary\n",
    "            landmarks_gestures_score = {\"Left\": [], \"Right\": []}\n",
    "            \n",
    "            # Check if landmarks exist for each pipeline and append them accordingly\n",
    "            if \"Left\" in first_pipeline_gesture_score_landmarks:\n",
    "                landmarks_gestures_score[\"Left\"].append(first_pipeline_gesture_score_landmarks[\"Left\"])\n",
    "            if \"Right\" in first_pipeline_gesture_score_landmarks:\n",
    "                landmarks_gestures_score[\"Right\"].append(first_pipeline_gesture_score_landmarks[\"Right\"])\n",
    "            \n",
    "            if \"Left\" in second_pipeline_gesture_score_landmarks:\n",
    "                landmarks_gestures_score[\"Left\"].append(second_pipeline_gesture_score_landmarks[\"Left\"])\n",
    "            if \"Right\" in second_pipeline_gesture_score_landmarks:\n",
    "                landmarks_gestures_score[\"Right\"].append(second_pipeline_gesture_score_landmarks[\"Right\"])\n",
    "            \n",
    "            if \"Left\" in third_pipeline_gesture_score_landmarks:\n",
    "                landmarks_gestures_score[\"Left\"].append(third_pipeline_gesture_score_landmarks[\"Left\"])\n",
    "            if \"Right\" in third_pipeline_gesture_score_landmarks:\n",
    "                landmarks_gestures_score[\"Right\"].append(third_pipeline_gesture_score_landmarks[\"Right\"])\n",
    "                \n",
    "            final_landmarks = []\n",
    "            \n",
    "            for hand in [\"Left\", \"Right\"]:\n",
    "                all_available_landmarks = landmarks_gestures_score[hand]\n",
    "                \n",
    "                if not all_available_landmarks:\n",
    "                    continue\n",
    "            \n",
    "                # Step 1: Filter only open palm gestures\n",
    "                open_palm_landmarks = [l for l in all_available_landmarks if l['gestures'] == 'Open_Palm']\n",
    "            \n",
    "                # Step 2: Handle multiple open palms\n",
    "                if len(open_palm_landmarks) > 1:\n",
    "                    merged_landmarks = merge_landmarks(open_palm_landmarks, hand)\n",
    "                    final_landmarks.append(merged_landmarks)\n",
    "            \n",
    "                # Step 3: Handle a single open palm\n",
    "                elif len(open_palm_landmarks) == 1:\n",
    "                    final_landmarks.append(open_palm_landmarks[0]['landmarks'])\n",
    "            \n",
    "                # Step 4: No open palm detected, use lowest score gesture\n",
    "                else:\n",
    "                    lowest_score_landmark = min(all_available_landmarks, key=lambda l: l['score'])\n",
    "                    final_landmarks.append(lowest_score_landmark['landmarks'])\n",
    "\n",
    "                        \n",
    "            final_landmark_accuracy = calculate_hand_accuracies(ground_truth_landmarks, final_landmarks, [\"Left\", \"Right\"], distance_threshold)\n",
    "            final_landmark_accuracy[\"Left\"] = [0.0]*21 if np.isnan(np.mean(final_landmark_accuracy[\"Left\"])) else final_landmark_accuracy[\"Left\"]\n",
    "            final_landmark_accuracy[\"Right\"] = [0.0]*21 if np.isnan(np.mean(final_landmark_accuracy[\"Right\"])) else final_landmark_accuracy[\"Right\"]\n",
    "            print(f\"Results for final score - Left: {np.mean(final_landmark_accuracy['Left']):.2f}%, Right: {np.mean(final_landmark_accuracy['Right']):.2f}%\")\n",
    "\n",
    "            # Annotate with final landmarks\n",
    "            annotated_image_path = f\"../../resources/stylized-pictures/annotated/{image_name}_annotated.png\"\n",
    "            annotate_landmarks(image_path, final_landmarks, annotated_image_path)\n",
    "\n",
    "            print(\"---------------------------------------------------------\")\n",
    "\n",
    "          \n",
    "            for hand in [\"Left\", \"Right\"]:\n",
    "                total_transformed_siggraph17[hand].extend(final_landmark_accuracy[hand])\n",
    "\n"
   ],
   "id": "bfcbe2fe5591f9d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image: 1_IMG20241127100244.jpg\n",
      "Results for original pipeline - Left: 23.37%, Right: 44.62%\n",
      "Results for second pipeline - Left: 65.81%, Right: 74.30%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/1_IMG20241127100244.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "2\n",
      "2\n",
      "Results for final score - Left: 64.28%, Right: 77.16%\n",
      "---------------------------------------------------------\n",
      "Processing image: 2_IMG20241127100252.jpg\n",
      "Results for original pipeline - Left: 36.07%, Right: 36.72%\n",
      "Results for second pipeline - Left: 65.22%, Right: 78.80%\n",
      "Results for third pipeline - Left: 53.30%, Right: 56.22%\n",
      "2\n",
      "Results for final score - Left: 65.22%, Right: 78.50%\n",
      "---------------------------------------------------------\n",
      "Processing image: 3_IMG20241127100408.jpg\n",
      "Results for original pipeline - Left: 76.93%, Right: 24.57%\n",
      "Results for second pipeline - Left: 76.77%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/3_IMG20241127100408.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "2\n",
      "Results for final score - Left: 76.37%, Right: 0.00%\n",
      "---------------------------------------------------------\n",
      "Processing image: 4_IMG20241127100414.jpg\n",
      "Results for original pipeline - Left: 76.92%, Right: 64.55%\n",
      "Results for second pipeline - Left: 0.00%, Right: 60.01%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/4_IMG20241127100414.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "2\n",
      "Results for final score - Left: 76.92%, Right: 72.05%\n",
      "---------------------------------------------------------\n",
      "Processing image: 5_IMG20241127100507.jpg\n",
      "Results for original pipeline - Left: 80.08%, Right: 77.84%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/5_IMG20241127100507.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/5_IMG20241127100507.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "Results for final score - Left: 80.08%, Right: 77.84%\n",
      "---------------------------------------------------------\n",
      "Processing image: 6_IMG20241127100515.jpg\n",
      "Results for original pipeline - Left: 80.77%, Right: 22.93%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/6_IMG20241127100515.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/6_IMG20241127100515.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "Results for final score - Left: 80.77%, Right: 22.93%\n",
      "---------------------------------------------------------\n",
      "Processing image: 7_IMG20241127100610.jpg\n",
      "Results for original pipeline - Left: 78.69%, Right: 76.72%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/7_IMG20241127100610.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/7_IMG20241127100610.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "Results for final score - Left: 78.69%, Right: 76.72%\n",
      "---------------------------------------------------------\n",
      "Processing image: 8_IMG20241127100617.jpg\n",
      "Results for original pipeline - Left: 79.28%, Right: 38.79%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/8_IMG20241127100617.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/8_IMG20241127100617.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "Results for final score - Left: 79.28%, Right: 38.79%\n",
      "---------------------------------------------------------\n",
      "Processing image: 9_IMG20241127100730.jpg\n",
      "Results for original pipeline - Left: 79.13%, Right: 75.47%\n",
      "Results for second pipeline - Left: 48.64%, Right: 21.73%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/9_IMG20241127100730.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "2\n",
      "Results for final score - Left: 74.22%, Right: 75.47%\n",
      "---------------------------------------------------------\n",
      "Processing image: 10_IMG20241127100740.jpg\n",
      "Results for original pipeline - Left: 76.76%, Right: 81.41%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/10_IMG20241127100740.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/10_IMG20241127100740.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "Results for final score - Left: 76.76%, Right: 81.41%\n",
      "---------------------------------------------------------\n",
      "Processing image: 11_IMG20241127100906.jpg\n",
      "Results for original pipeline - Left: 80.80%, Right: 76.51%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/11_IMG20241127100906.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/11_IMG20241127100906.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "Results for final score - Left: 80.80%, Right: 76.51%\n",
      "---------------------------------------------------------\n",
      "Processing image: 12_IMG20241127100913.jpg\n",
      "Results for original pipeline - Left: 78.56%, Right: 76.66%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/12_IMG20241127100913.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/12_IMG20241127100913.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "Results for final score - Left: 78.56%, Right: 76.66%\n",
      "---------------------------------------------------------\n",
      "Processing image: 13_IMG20241127101012.jpg\n",
      "Results for original pipeline - Left: 79.56%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/13_IMG20241127101012.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/13_IMG20241127101012.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "Results for final score - Left: 79.56%, Right: 0.00%\n",
      "---------------------------------------------------------\n",
      "Processing image: 14_IMG20241127101019.jpg\n",
      "Results for original pipeline - Left: 83.02%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/14_IMG20241127101019.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/14_IMG20241127101019.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "Results for final score - Left: 83.02%, Right: 0.00%\n",
      "---------------------------------------------------------\n",
      "Processing image: 15_IMG20241127101422.jpg\n",
      "Results for original pipeline - Left: 70.67%, Right: 70.91%\n",
      "Results for second pipeline - Left: 66.06%, Right: 73.15%\n",
      "Results for third pipeline - Left: 69.79%, Right: 75.74%\n",
      "3\n",
      "3\n",
      "Results for final score - Left: 66.05%, Right: 72.20%\n",
      "---------------------------------------------------------\n",
      "Processing image: 16_IMG20241127101432.jpg\n",
      "Results for original pipeline - Left: 75.98%, Right: 73.17%\n",
      "Results for second pipeline - Left: 74.67%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/16_IMG20241127101432.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "2\n",
      "Results for final score - Left: 74.14%, Right: 73.17%\n",
      "---------------------------------------------------------\n",
      "Processing image: 17_IMG20241127101442.jpg\n",
      "Results for original pipeline - Left: 66.82%, Right: 65.71%\n",
      "Results for second pipeline - Left: 65.23%, Right: 69.63%\n",
      "Results for third pipeline - Left: 67.93%, Right: 0.00%\n",
      "3\n",
      "2\n",
      "Results for final score - Left: 64.44%, Right: 64.45%\n",
      "---------------------------------------------------------\n",
      "Processing image: 18_IMG20241127101447.jpg\n",
      "Results for original pipeline - Left: 71.71%, Right: 66.93%\n",
      "Results for second pipeline - Left: 63.19%, Right: 68.56%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/18_IMG20241127101447.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "2\n",
      "2\n",
      "Results for final score - Left: 66.57%, Right: 68.37%\n",
      "---------------------------------------------------------\n",
      "Processing image: 19_IMG20241127101600.jpg\n",
      "Results for original pipeline - Left: 64.71%, Right: 70.59%\n",
      "Results for second pipeline - Left: 59.60%, Right: 68.08%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/19_IMG20241127101600.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "2\n",
      "2\n",
      "Results for final score - Left: 59.95%, Right: 68.66%\n",
      "---------------------------------------------------------\n",
      "Processing image: 20_IMG20241127101607.jpg\n",
      "Results for original pipeline - Left: 68.91%, Right: 67.74%\n",
      "Results for second pipeline - Left: 67.52%, Right: 69.22%\n",
      "Results for third pipeline - Left: 66.11%, Right: 69.32%\n",
      "3\n",
      "3\n",
      "Results for final score - Left: 68.77%, Right: 67.94%\n",
      "---------------------------------------------------------\n",
      "Processing image: 21_IMG20241127101704.jpg\n",
      "Results for original pipeline - Left: 67.45%, Right: 72.39%\n",
      "Results for second pipeline - Left: 66.53%, Right: 71.97%\n",
      "Results for third pipeline - Left: 63.84%, Right: 75.67%\n",
      "3\n",
      "3\n",
      "Results for final score - Left: 65.02%, Right: 69.59%\n",
      "---------------------------------------------------------\n",
      "Processing image: 22_IMG20241127101709.jpg\n",
      "Results for original pipeline - Left: 71.76%, Right: 69.47%\n",
      "Results for second pipeline - Left: 68.00%, Right: 75.80%\n",
      "Results for third pipeline - Left: 65.79%, Right: 0.00%\n",
      "3\n",
      "2\n",
      "Results for final score - Left: 69.30%, Right: 73.03%\n",
      "---------------------------------------------------------\n",
      "Processing image: 23_IMG20241127101749.jpg\n",
      "Results for original pipeline - Left: 57.62%, Right: 74.97%\n",
      "Results for second pipeline - Left: 66.10%, Right: 71.24%\n",
      "Results for third pipeline - Left: 0.00%, Right: 59.68%\n",
      "2\n",
      "2\n",
      "Results for final score - Left: 65.14%, Right: 70.69%\n",
      "---------------------------------------------------------\n",
      "Processing image: 24_IMG20241127101754.jpg\n",
      "Results for original pipeline - Left: 69.39%, Right: 70.76%\n",
      "Results for second pipeline - Left: 0.00%, Right: 66.18%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/24_IMG20241127101754.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "2\n",
      "Results for final score - Left: 69.39%, Right: 65.87%\n",
      "---------------------------------------------------------\n",
      "Processing image: 25_IMG20241127101835.jpg\n",
      "Results for original pipeline - Left: 57.21%, Right: 71.22%\n",
      "Results for second pipeline - Left: 61.86%, Right: 79.25%\n",
      "Results for third pipeline - Left: 0.00%, Right: 73.46%\n",
      "2\n",
      "3\n",
      "Results for final score - Left: 56.01%, Right: 67.73%\n",
      "---------------------------------------------------------\n",
      "Processing image: 26_IMG20241127101840.jpg\n",
      "Results for original pipeline - Left: 59.95%, Right: 67.03%\n",
      "Results for second pipeline - Left: 66.11%, Right: 75.96%\n",
      "Results for third pipeline - Left: 0.00%, Right: 72.93%\n",
      "2\n",
      "3\n",
      "Results for final score - Left: 58.16%, Right: 68.17%\n",
      "---------------------------------------------------------\n",
      "Processing image: 27_IMG20241127101939.jpg\n",
      "Results for original pipeline - Left: 65.07%, Right: 57.32%\n",
      "Results for second pipeline - Left: 0.00%, Right: 74.97%\n",
      "Results for third pipeline - Left: 0.00%, Right: 71.09%\n",
      "3\n",
      "Results for final score - Left: 65.07%, Right: 56.00%\n",
      "---------------------------------------------------------\n",
      "Processing image: 28_IMG20241127101946.jpg\n",
      "Results for original pipeline - Left: 59.35%, Right: 69.51%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/28_IMG20241127101946.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "Results for third pipeline - Left: 0.00%, Right: 70.45%\n",
      "2\n",
      "Results for final score - Left: 59.35%, Right: 65.75%\n",
      "---------------------------------------------------------\n",
      "Processing image: 29_IMG20241127102051.jpg\n",
      "Results for original pipeline - Left: 0.00%, Right: 46.89%\n",
      "Results for second pipeline - Left: 66.57%, Right: 40.76%\n",
      "Results for third pipeline - Left: 43.61%, Right: 0.00%\n",
      "2\n",
      "Results for final score - Left: 66.11%, Right: 40.76%\n",
      "---------------------------------------------------------\n",
      "Processing image: 30_IMG20241127102058.jpg\n",
      "Results for original pipeline - Left: 19.09%, Right: 29.62%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/30_IMG20241127102058.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "Results for third pipeline - Left: 60.44%, Right: 59.50%\n",
      "Results for final score - Left: 60.44%, Right: 59.50%\n",
      "---------------------------------------------------------\n",
      "Processing image: 31_IMG20241127102205.jpg\n",
      "Results for original pipeline - Left: 71.01%, Right: 80.12%\n",
      "Results for second pipeline - Left: 73.54%, Right: 84.99%\n",
      "Results for third pipeline - Left: 73.02%, Right: 81.10%\n",
      "3\n",
      "3\n",
      "Results for final score - Left: 75.82%, Right: 85.41%\n",
      "---------------------------------------------------------\n",
      "Processing image: 32_IMG20241127102216.jpg\n",
      "Results for original pipeline - Left: 73.54%, Right: 76.13%\n",
      "Results for second pipeline - Left: 77.72%, Right: 77.44%\n",
      "Results for third pipeline - Left: 74.84%, Right: 80.17%\n",
      "3\n",
      "3\n",
      "Results for final score - Left: 78.73%, Right: 82.16%\n",
      "---------------------------------------------------------\n",
      "Processing image: 33_IMG20241127102317.jpg\n",
      "Results for original pipeline - Left: 51.10%, Right: 65.87%\n",
      "Results for second pipeline - Left: 65.63%, Right: 73.25%\n",
      "Results for third pipeline - Left: 0.00%, Right: 72.11%\n",
      "2\n",
      "3\n",
      "Results for final score - Left: 68.17%, Right: 73.68%\n",
      "---------------------------------------------------------\n",
      "Processing image: 34_IMG20241127102423.jpg\n",
      "Results for original pipeline - Left: 66.14%, Right: 67.82%\n",
      "Results for second pipeline - Left: 73.90%, Right: 78.97%\n",
      "Results for third pipeline - Left: 0.00%, Right: 70.31%\n",
      "2\n",
      "2\n",
      "Results for final score - Left: 73.90%, Right: 79.35%\n",
      "---------------------------------------------------------\n",
      "Processing image: 35_IMG20241127102430.jpg\n",
      "Results for original pipeline - Left: 56.35%, Right: 67.90%\n",
      "Results for second pipeline - Left: 70.99%, Right: 73.86%\n",
      "Results for third pipeline - Left: 0.00%, Right: 73.10%\n",
      "2\n",
      "Results for final score - Left: 70.99%, Right: 72.72%\n",
      "---------------------------------------------------------\n",
      "Processing image: 36_IMG20241127102532.jpg\n",
      "Results for original pipeline - Left: 67.77%, Right: 42.86%\n",
      "Results for second pipeline - Left: 0.00%, Right: 80.43%\n",
      "Results for third pipeline - Left: 59.57%, Right: 81.57%\n",
      "2\n",
      "Results for final score - Left: 67.77%, Right: 81.81%\n",
      "---------------------------------------------------------\n",
      "Processing image: 37_IMG20241127102538.jpg\n",
      "Results for original pipeline - Left: 62.34%, Right: 40.92%\n",
      "Results for second pipeline - Left: 0.00%, Right: 77.94%\n",
      "Results for third pipeline - Left: 0.00%, Right: 60.80%\n",
      "2\n",
      "Results for final score - Left: 62.34%, Right: 78.15%\n",
      "---------------------------------------------------------\n",
      "Processing image: 38_IMG20241127102643.jpg\n",
      "Results for original pipeline - Left: 28.45%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/38_IMG20241127102643.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/38_IMG20241127102643.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "Results for final score - Left: 28.45%, Right: 0.00%\n",
      "---------------------------------------------------------\n",
      "Processing image: 39_IMG20241127102649.jpg\n",
      "Results for original pipeline - Left: 34.05%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/39_IMG20241127102649.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/39_IMG20241127102649.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "Results for final score - Left: 34.05%, Right: 0.00%\n",
      "---------------------------------------------------------\n",
      "Processing image: 40_IMG20241127102805.jpg\n",
      "No hands detected for ../../resources/stylized-pictures/siggraph17/40_IMG20241127102805.jpg_siggraph17.png\n",
      "Results for original pipeline - Left: 0.00%, Right: 0.00%\n",
      "Results for second pipeline - Left: 0.00%, Right: 75.93%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/40_IMG20241127102805.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "Results for final score - Left: 0.00%, Right: 75.93%\n",
      "---------------------------------------------------------\n",
      "Processing image: 41_IMG20241127102811.jpg\n",
      "Results for original pipeline - Left: 70.15%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/41_IMG20241127102811.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/41_IMG20241127102811.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "Results for final score - Left: 70.15%, Right: 0.00%\n",
      "---------------------------------------------------------\n",
      "Processing image: 42_IMG20241127102854.jpg\n",
      "Results for original pipeline - Left: 66.65%, Right: 78.16%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/42_IMG20241127102854.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/42_IMG20241127102854.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "Results for final score - Left: 66.65%, Right: 78.16%\n",
      "---------------------------------------------------------\n",
      "Processing image: 43_IMG20241127102901.jpg\n",
      "Results for original pipeline - Left: 81.75%, Right: 75.00%\n",
      "Results for second pipeline - Left: 70.18%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/43_IMG20241127102901.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "2\n",
      "Results for final score - Left: 74.37%, Right: 75.00%\n",
      "---------------------------------------------------------\n",
      "Processing image: 44_IMG20241127103107.jpg\n",
      "Results for original pipeline - Left: 74.57%, Right: 78.91%\n",
      "Results for second pipeline - Left: 72.50%, Right: 50.69%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/44_IMG20241127103107.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "2\n",
      "2\n",
      "Results for final score - Left: 74.43%, Right: 74.61%\n",
      "---------------------------------------------------------\n",
      "Processing image: 45_IMG20241127103113.jpg\n",
      "Results for original pipeline - Left: 80.90%, Right: 85.30%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/45_IMG20241127103113.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/45_IMG20241127103113.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "Results for final score - Left: 80.90%, Right: 85.30%\n",
      "---------------------------------------------------------\n",
      "Processing image: 46_IMG20241127103125.jpg\n",
      "Results for original pipeline - Left: 79.79%, Right: 83.69%\n",
      "Results for second pipeline - Left: 0.00%, Right: 80.29%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/46_IMG20241127103125.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "2\n",
      "Results for final score - Left: 79.79%, Right: 82.99%\n",
      "---------------------------------------------------------\n",
      "Processing image: 47_IMG20241127103130.jpg\n",
      "Results for original pipeline - Left: 81.88%, Right: 85.07%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/47_IMG20241127103130.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/47_IMG20241127103130.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "Results for final score - Left: 81.88%, Right: 85.07%\n",
      "---------------------------------------------------------\n",
      "Processing image: 48_IMG20241127103201.jpg\n",
      "Results for original pipeline - Left: 83.80%, Right: 85.21%\n",
      "Results for second pipeline - Left: 0.00%, Right: 81.91%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/48_IMG20241127103201.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "2\n",
      "Results for final score - Left: 83.80%, Right: 83.19%\n",
      "---------------------------------------------------------\n",
      "Processing image: 49_IMG20241127103207.jpg\n",
      "Results for original pipeline - Left: 80.91%, Right: 81.05%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/49_IMG20241127103207.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/49_IMG20241127103207.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "Results for final score - Left: 80.91%, Right: 81.05%\n",
      "---------------------------------------------------------\n",
      "Processing image: 50_IMG20241127103243.jpg\n",
      "Results for original pipeline - Left: 78.95%, Right: 81.61%\n",
      "Results for second pipeline - Left: 0.00%, Right: 81.30%\n",
      "Results for third pipeline - Left: 79.54%, Right: 83.90%\n",
      "2\n",
      "3\n",
      "Results for final score - Left: 79.48%, Right: 81.51%\n",
      "---------------------------------------------------------\n",
      "Processing image: 51_IMG20241127103250.jpg\n",
      "Results for original pipeline - Left: 77.57%, Right: 82.56%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/51_IMG20241127103250.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/51_IMG20241127103250.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "Results for final score - Left: 77.57%, Right: 82.56%\n",
      "---------------------------------------------------------\n",
      "Processing image: 52_IMG20241127103332.jpg\n",
      "Results for original pipeline - Left: 74.55%, Right: 79.69%\n",
      "Results for second pipeline - Left: 77.29%, Right: 78.52%\n",
      "Results for third pipeline - Left: 0.00%, Right: 71.67%\n",
      "2\n",
      "3\n",
      "Results for final score - Left: 77.27%, Right: 71.72%\n",
      "---------------------------------------------------------\n",
      "Processing image: 53_IMG20241127103337.jpg\n",
      "Results for original pipeline - Left: 74.99%, Right: 80.18%\n",
      "Results for second pipeline - Left: 76.99%, Right: 79.06%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/53_IMG20241127103337.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "2\n",
      "2\n",
      "Results for final score - Left: 77.94%, Right: 81.39%\n",
      "---------------------------------------------------------\n",
      "Processing image: 54_IMG20241127103409.jpg\n",
      "Results for original pipeline - Left: 55.67%, Right: 57.44%\n",
      "Results for second pipeline - Left: 0.00%, Right: 80.41%\n",
      "Results for third pipeline - Left: 65.54%, Right: 77.20%\n",
      "2\n",
      "Results for final score - Left: 65.54%, Right: 76.98%\n",
      "---------------------------------------------------------\n",
      "Processing image: 55_IMG20241127103413.jpg\n",
      "Results for original pipeline - Left: 69.63%, Right: 52.88%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/55_IMG20241127103413.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/55_IMG20241127103413.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "Results for final score - Left: 69.63%, Right: 52.88%\n",
      "---------------------------------------------------------\n",
      "Processing image: 56_IMG20241127103449.jpg\n",
      "Results for original pipeline - Left: 68.46%, Right: 33.46%\n",
      "Results for second pipeline - Left: 0.00%, Right: 75.97%\n",
      "Results for third pipeline - Left: 57.05%, Right: 78.59%\n",
      "2\n",
      "2\n",
      "Results for final score - Left: 77.51%, Right: 77.44%\n",
      "---------------------------------------------------------\n",
      "Processing image: 57_IMG20241127103455.jpg\n",
      "Results for original pipeline - Left: 0.00%, Right: 33.27%\n",
      "Results for second pipeline - Left: 64.03%, Right: 66.35%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/57_IMG20241127103455.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "Results for final score - Left: 64.03%, Right: 66.35%\n",
      "---------------------------------------------------------\n",
      "Processing image: 58_IMG20241127103532.jpg\n",
      "Results for original pipeline - Left: 69.35%, Right: 60.43%\n",
      "Results for second pipeline - Left: 62.54%, Right: 47.44%\n",
      "Results for third pipeline - Left: 61.08%, Right: 48.19%\n",
      "3\n",
      "Results for final score - Left: 69.38%, Right: 60.43%\n",
      "---------------------------------------------------------\n",
      "Processing image: 59_IMG20241127103541.jpg\n",
      "Results for original pipeline - Left: 24.97%, Right: 35.32%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/59_IMG20241127103541.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/59_IMG20241127103541.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "Results for final score - Left: 24.97%, Right: 35.32%\n",
      "---------------------------------------------------------\n",
      "Processing image: 60_IMG20241127103613.jpg\n",
      "Results for original pipeline - Left: 78.58%, Right: 80.70%\n",
      "Results for second pipeline - Left: 82.46%, Right: 83.14%\n",
      "Results for third pipeline - Left: 69.22%, Right: 0.00%\n",
      "3\n",
      "2\n",
      "Results for final score - Left: 83.23%, Right: 83.94%\n",
      "---------------------------------------------------------\n",
      "Processing image: 61_IMG20241127103622.jpg\n",
      "Results for original pipeline - Left: 73.50%, Right: 81.53%\n",
      "Results for second pipeline - Left: 0.00%, Right: 85.06%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/61_IMG20241127103622.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "2\n",
      "Results for final score - Left: 73.50%, Right: 86.13%\n",
      "---------------------------------------------------------\n",
      "Processing image: 62_IMG20241127103640.jpg\n",
      "Results for original pipeline - Left: 79.95%, Right: 79.26%\n",
      "Results for second pipeline - Left: 82.32%, Right: 84.57%\n",
      "Results for third pipeline - Left: 82.16%, Right: 83.46%\n",
      "2\n",
      "3\n",
      "Results for final score - Left: 84.82%, Right: 83.43%\n",
      "---------------------------------------------------------\n",
      "Processing image: 63_IMG20241127103645.jpg\n",
      "Results for original pipeline - Left: 76.54%, Right: 80.14%\n",
      "Results for second pipeline - Left: 75.12%, Right: 79.80%\n",
      "Results for third pipeline - Left: 69.17%, Right: 80.43%\n",
      "3\n",
      "3\n",
      "Results for final score - Left: 76.25%, Right: 81.15%\n",
      "---------------------------------------------------------\n",
      "Processing image: 64_IMG20241127103711.jpg\n",
      "Results for original pipeline - Left: 80.22%, Right: 83.38%\n",
      "Results for second pipeline - Left: 42.67%, Right: 84.64%\n",
      "Results for third pipeline - Left: 3.30%, Right: 86.29%\n",
      "3\n",
      "Results for final score - Left: 80.22%, Right: 86.07%\n",
      "---------------------------------------------------------\n",
      "Processing image: 65_IMG20241127103717.jpg\n",
      "Results for original pipeline - Left: 81.95%, Right: 76.18%\n",
      "Results for second pipeline - Left: 0.00%, Right: 79.36%\n",
      "Results for third pipeline - Left: 67.97%, Right: 73.55%\n",
      "2\n",
      "3\n",
      "Results for final score - Left: 81.87%, Right: 78.03%\n",
      "---------------------------------------------------------\n",
      "Processing image: 66_IMG20241127103745.jpg\n",
      "Results for original pipeline - Left: 52.10%, Right: 75.85%\n",
      "Results for second pipeline - Left: 70.80%, Right: 75.14%\n",
      "Results for third pipeline - Left: 70.83%, Right: 75.12%\n",
      "2\n",
      "3\n",
      "Results for final score - Left: 72.97%, Right: 75.80%\n",
      "---------------------------------------------------------\n",
      "Processing image: 67_IMG20241127103751.jpg\n",
      "Results for original pipeline - Left: 79.04%, Right: 84.64%\n",
      "Results for second pipeline - Left: 83.91%, Right: 84.90%\n",
      "Results for third pipeline - Left: 65.14%, Right: 85.10%\n",
      "3\n",
      "3\n",
      "Results for final score - Left: 84.12%, Right: 84.99%\n",
      "---------------------------------------------------------\n",
      "Processing image: 68_IMG20241127103819.jpg\n",
      "Results for original pipeline - Left: 37.54%, Right: 60.95%\n",
      "Results for second pipeline - Left: 56.17%, Right: 72.94%\n",
      "Results for third pipeline - Left: 6.91%, Right: 79.88%\n",
      "2\n",
      "Results for final score - Left: 56.17%, Right: 79.77%\n",
      "---------------------------------------------------------\n",
      "Processing image: 69_IMG20241127103825.jpg\n",
      "Results for original pipeline - Left: 49.90%, Right: 77.84%\n",
      "Results for second pipeline - Left: 71.42%, Right: 82.13%\n",
      "Results for third pipeline - Left: 60.89%, Right: 78.80%\n",
      "3\n",
      "3\n",
      "Results for final score - Left: 69.70%, Right: 82.03%\n",
      "---------------------------------------------------------\n",
      "Processing image: 70_IMG20241127103904.jpg\n",
      "Results for original pipeline - Left: 21.02%, Right: 42.09%\n",
      "Results for second pipeline - Left: 0.00%, Right: 41.77%\n",
      "Results for third pipeline - Left: 5.14%, Right: 76.91%\n",
      "2\n",
      "Results for final score - Left: 5.14%, Right: 76.91%\n",
      "---------------------------------------------------------\n",
      "Processing image: 71_IMG20241127103908.jpg\n",
      "Results for original pipeline - Left: 3.63%, Right: 55.37%\n",
      "Results for second pipeline - Left: 56.40%, Right: 76.60%\n",
      "Results for third pipeline - Left: 48.67%, Right: 74.49%\n",
      "3\n",
      "Results for final score - Left: 3.63%, Right: 77.94%\n",
      "---------------------------------------------------------\n",
      "Processing image: 72_IMG20241127103941.jpg\n",
      "Results for original pipeline - Left: 14.70%, Right: 6.48%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/72_IMG20241127103941.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "Results for third pipeline - Left: 64.15%, Right: 61.28%\n",
      "Results for final score - Left: 64.15%, Right: 61.28%\n",
      "---------------------------------------------------------\n",
      "Processing image: 73_IMG20241127103945.jpg\n",
      "Results for original pipeline - Left: 8.31%, Right: 12.19%\n",
      "Results for second pipeline - Left: 0.00%, Right: 53.71%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/73_IMG20241127103945.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "Results for final score - Left: 8.31%, Right: 53.71%\n",
      "---------------------------------------------------------\n",
      "Processing image: 74_IMG20241127104014.jpg\n",
      "No hands detected for ../../resources/stylized-pictures/siggraph17/74_IMG20241127104014.jpg_siggraph17.png\n",
      "Results for original pipeline - Left: 0.00%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/74_IMG20241127104014.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "Results for third pipeline - Left: 0.00%, Right: 58.35%\n",
      "Results for final score - Left: 0.00%, Right: 58.35%\n",
      "---------------------------------------------------------\n",
      "Processing image: 75_IMG20241127104020.jpg\n",
      "No hands detected for ../../resources/stylized-pictures/siggraph17/75_IMG20241127104020.jpg_siggraph17.png\n",
      "Results for original pipeline - Left: 0.00%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/75_IMG20241127104020.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/75_IMG20241127104020.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "Results for final score - Left: 0.00%, Right: 0.00%\n",
      "---------------------------------------------------------\n",
      "Processing image: 76_IMG20241127104628.jpg\n",
      "Results for original pipeline - Left: 80.17%, Right: 75.45%\n",
      "Results for second pipeline - Left: 81.45%, Right: 77.37%\n",
      "Results for third pipeline - Left: 78.25%, Right: 79.39%\n",
      "3\n",
      "3\n",
      "Results for final score - Left: 79.65%, Right: 79.96%\n",
      "---------------------------------------------------------\n",
      "Processing image: 77_IMG20241127104637.jpg\n",
      "Results for original pipeline - Left: 81.97%, Right: 79.10%\n",
      "Results for second pipeline - Left: 56.63%, Right: 73.95%\n",
      "Results for third pipeline - Left: 77.44%, Right: 0.00%\n",
      "3\n",
      "2\n",
      "Results for final score - Left: 75.18%, Right: 75.22%\n",
      "---------------------------------------------------------\n",
      "Processing image: 78_IMG20241127105250.jpg\n",
      "Results for original pipeline - Left: 0.00%, Right: 73.28%\n",
      "Results for second pipeline - Left: 13.39%, Right: 69.64%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/78_IMG20241127105250.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "2\n",
      "Results for final score - Left: 13.39%, Right: 70.08%\n",
      "---------------------------------------------------------\n",
      "Processing image: 79_IMG20241127105256.jpg\n",
      "Results for original pipeline - Left: 63.81%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/not_detected/sharpened/79_IMG20241127105256.jpg_sharpened.png\n",
      "Results for second pipeline - Left: 0.00%, Right: 0.00%\n",
      "No hands detected for ../../resources/stylized-pictures/enhanced/79_IMG20241127105256.jpg_enhanced.png\n",
      "Results for third pipeline - Left: 0.00%, Right: 0.00%\n",
      "Results for final score - Left: 63.81%, Right: 0.00%\n",
      "---------------------------------------------------------\n",
      "Processing image: 80_IMG20241127105346.jpg\n",
      "Results for original pipeline - Left: 13.17%, Right: 69.18%\n",
      "Results for second pipeline - Left: 11.97%, Right: 72.68%\n",
      "Results for third pipeline - Left: 70.32%, Right: 62.28%\n",
      "1\n",
      "3\n",
      "Results for final score - Left: 70.32%, Right: 74.87%\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 232
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T13:33:13.734352Z",
     "start_time": "2025-01-06T13:33:13.718713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transformed_accuracy_siggraph17 = calculate_final_accuracy(total_transformed_siggraph17)\n",
    "# transformed_accuracy_eccv16 = calculate_final_accuracy(total_transformed_eccv16)"
   ],
   "id": "e65a45a34689ca5a",
   "outputs": [],
   "execution_count": 233
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T13:33:13.789116Z",
     "start_time": "2025-01-06T13:33:13.773491Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Transformed Accuracy (Siggraph17): {transformed_accuracy_siggraph17:.2f}%\")",
   "id": "d035ac7d6a6a6810",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Accuracy (Siggraph17): 65.05%\n"
     ]
    }
   ],
   "execution_count": 234
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:11:08.344423Z",
     "start_time": "2025-01-06T11:11:08.328806Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "44d4e52ad261c31d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
